import streamlit as st
import jieba
from collections import Counter
import numpy as np
from snownlp import SnowNLP
import pandas as pd

# é¡µé¢é…ç½®
st.set_page_config(page_title="å¢å¼ºç‰ˆæ–‡æœ¬åˆ†æå·¥å…·", page_icon="ğŸ“", layout="centered")

# æ‰©å……åœç”¨è¯è¡¨
STOP_WORDS = {
    "çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ",
    "åœ¨", "å’Œ", "æœ‰", "å°±", "éƒ½", "è¿™", "é‚£", "å…¶",
    "ä¹‹", "äº", "ä»¥", "ä¸º", "è€Œ", "ä¹Ÿ", "å—", "å‘¢",
    "å§", "å•Š", "å“¦", "å—¯", "ç€", "è¿‡", "è¿˜", "å°†",
    "è¦", "ä¼š", "èƒ½", "å¯", "å¯¹", "ä¸", "æˆ–", "åŠ",
    "æ‰€", "æŠŠ", "è¢«", "è®©", "ç»™", "ä½¿", "å¾—", "åˆ°",
    "ä»", "å¾€", "å‘", "æ¯”", "è·Ÿ", "åŒ", "å’Œ"
}

# ---------------------- æ ¸å¿ƒå‡½æ•°ï¼ˆç§»é™¤å¤æ‚Matplotlibä¾èµ–ï¼Œæ”¹ç”¨åŸç”Ÿç»„ä»¶ï¼‰ ----------------------
def calculate_text_stats(input_text):
    total_with_space = len(input_text)
    pure_text = input_text.replace(" ", "").replace("\n", "")
    total_without_space = len(pure_text)
    
    sentence_end_chars = "ã€‚ï¼ï¼Ÿï¼›"
    sentence_count = 1
    for char in sentence_end_chars:
        sentence_count += pure_text.count(char)
    
    punctuation_chars = 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹,.!?;:\'"()[]{}<>ã€'
    punctuation_count = sum(1 for char in pure_text if char in punctuation_chars)
    pure_word_count = total_without_space - punctuation_count
    
    return {
        "å«ç©ºæ ¼æ¢è¡Œæ€»å­—ç¬¦æ•°": total_with_space,
        "æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°": total_without_space,
        "çº¯æ–‡æœ¬å†…å®¹": pure_text,
        "å¥å­æ•°": sentence_count,
        "æ ‡ç‚¹ç¬¦å·æ•°": punctuation_count,
        "çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰": pure_word_count
    }

def get_top_keywords(pure_text, top_n=10):
    if not pure_text:
        return []
    word_list = jieba.lcut(pure_text)
    valid_words = [
        word for word in word_list
        if word not in STOP_WORDS and len(word) > 1 and word.strip()
    ]
    if not valid_words:
        return []
    word_count = Counter(valid_words)
    return word_count.most_common(top_n)

def analyze_sentiment(pure_text):
    if not pure_text:
        return {"æƒ…æ„Ÿå¾—åˆ†": 0.5, "æƒ…æ„Ÿå€¾å‘": "ä¸­æ€§", "æ–‡æœ¬æ‘˜è¦": []}
    s = SnowNLP(pure_text)
    sentiment_score = s.sentiments
    if sentiment_score >= 0.7:
        sentiment_tendency = "æ­£é¢"
    elif sentiment_score <= 0.3:
        sentiment_tendency = "è´Ÿé¢"
    else:
        sentiment_tendency = "ä¸­æ€§"
    summary_list = s.summary(3) if len(pure_text) > 10 else ["æ–‡æœ¬è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦"]
    return {
        "æƒ…æ„Ÿå¾—åˆ†": round(sentiment_score, 4),
        "æƒ…æ„Ÿå€¾å‘": sentiment_tendency,
        "æ–‡æœ¬æ‘˜è¦": summary_list
    }

def get_word_segmentation(pure_text):
    if not pure_text:
        return "æ— æœ‰æ•ˆæ–‡æœ¬"
    word_list = jieba.lcut(pure_text)
    filtered_word_list = [word for word in word_list if word not in STOP_WORDS and word.strip()]
    if not filtered_word_list:
        return "æ— æœ‰æ•ˆåˆ†è¯ï¼ˆå…¨ä¸ºåœç”¨è¯/æ ‡ç‚¹ï¼‰"
    return " | ".join(filtered_word_list)

# æ›¿æ¢Matplotlibé¥¼å›¾ï¼šç”¨StreamlitåŸç”Ÿè¡¨æ ¼+è¿›åº¦æ¡å±•ç¤ºæ–‡æœ¬æ„æˆ
def show_text_composition(text_stats):
    pure_word_count = text_stats["çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰"]
    punctuation_count = text_stats["æ ‡ç‚¹ç¬¦å·æ•°"]
    total = pure_word_count + punctuation_count
    
    if total == 0:
        st.info("ğŸ“Œ æ— æœ‰æ•ˆæ–‡æœ¬æ•°æ®å¯å±•ç¤º")
        return
    
    # è®¡ç®—å æ¯”
    word_ratio = round((pure_word_count / total) * 100, 1)
    punctuation_ratio = round((punctuation_count / total) * 100, 1)
    
    # ç”¨è¡¨æ ¼å±•ç¤ºå æ¯”
    comp_data = pd.DataFrame({
        "æ–‡æœ¬ç±»å‹": ["çº¯æ–‡å­—", "æ ‡ç‚¹ç¬¦å·"],
        "æ•°é‡": [pure_word_count, punctuation_count],
        "å æ¯”(%)": [word_ratio, punctuation_ratio]
    })
    st.table(comp_data)
    
    # ç”¨è¿›åº¦æ¡å¯è§†åŒ–å æ¯”
    st.write("### å æ¯”å¯è§†åŒ–")
    col1, col2 = st.columns(2)
    with col1:
        st.write(f"çº¯æ–‡å­—ï¼ˆ{word_ratio}%ï¼‰")
        st.progress(word_ratio / 100)
    with col2:
        st.write(f"æ ‡ç‚¹ç¬¦å·ï¼ˆ{punctuation_ratio}%ï¼‰")
        st.progress(punctuation_ratio / 100)

# æ›¿æ¢Matplotlibæƒ…æ„Ÿå‚è€ƒå›¾ï¼šç”¨åŸç”Ÿæ–‡å­—+æ ‡ç­¾å±•ç¤ºæƒ…æ„ŸåŒºé—´
def show_sentiment_reference(sentiment_score):
    st.write("### æƒ…æ„Ÿå¾—åˆ†åŒºé—´è¯´æ˜")
    # ç”¨markdownå±•ç¤ºåŒºé—´ï¼Œå¤©ç„¶æ”¯æŒä¸­æ–‡
    st.markdown("""
    | å¾—åˆ†åŒºé—´ | æƒ…æ„Ÿå€¾å‘ |
    |----------|----------|
    | 0.0 - 0.3 | è´Ÿé¢ |
    | 0.3 - 0.7 | ä¸­æ€§ |
    | 0.7 - 1.0 | æ­£é¢ |
    """)
    
    # å±•ç¤ºå½“å‰å¾—åˆ†å’Œå€¾å‘
    sentiment_label = "æ­£é¢" if sentiment_score >=0.7 else "è´Ÿé¢" if sentiment_score <=0.3 else "ä¸­æ€§"
    st.write(f"#### å½“å‰æ–‡æœ¬ï¼š{sentiment_label}ï¼ˆå¾—åˆ†ï¼š{sentiment_score}ï¼‰")
    
    # ç”¨å½©è‰²æ ‡ç­¾çªå‡ºæ˜¾ç¤º
    if sentiment_label == "æ­£é¢":
        st.success(f"âœ… æƒ…æ„Ÿå€¾å‘ï¼š{sentiment_label}")
    elif sentiment_label == "è´Ÿé¢":
        st.error(f"âŒ æƒ…æ„Ÿå€¾å‘ï¼š{sentiment_label}")
    else:
        st.info(f"â„¹ï¸ æƒ…æ„Ÿå€¾å‘ï¼š{sentiment_label}")

# ç®€åŒ–è¯äº‘å›¾ï¼šè‹¥æ— æ³•æ˜¾ç¤ºä¸­æ–‡ï¼Œæ›¿æ¢ä¸ºå…³é”®è¯æƒé‡åˆ—è¡¨
def show_wordcloud_alternative(pure_text):
    st.subheader("â˜ï¸ å…³é”®è¯æƒé‡å±•ç¤ºï¼ˆæ›¿ä»£è¯äº‘å›¾ï¼Œä¸­æ–‡æ¸…æ™°æ˜¾ç¤ºï¼‰")
    top_keywords = get_top_keywords(pure_text, top_n=20)
    if not top_keywords:
        st.info("ğŸ“Œ æ— æœ‰æ•ˆå…³é”®è¯å¯å±•ç¤º")
        return
    
    # ç”¨å¸¦æ ·å¼çš„åˆ—è¡¨å±•ç¤ºå…³é”®è¯ï¼ˆæŒ‰å‡ºç°æ¬¡æ•°æ’åºï¼Œå­—ä½“å¤§å°åŒºåˆ†æƒé‡ï¼‰
    for word, count in top_keywords:
        # å‡ºç°æ¬¡æ•°è¶Šå¤šï¼Œå­—ä½“è¶Šå¤§
        font_size = min(12 + count * 2, 20)  # é™åˆ¶æœ€å¤§å­—ä½“
        st.markdown(f"<span style='font-size:{font_size}px; color:#2E86AB; font-weight:bold;'>{word}</span> ï¼ˆå‡ºç°{count}æ¬¡ï¼‰", unsafe_allow_html=True)

# ---------------------- é¡µé¢äº¤äº’ï¼ˆå…¨æ¨¡å—ä¸­æ–‡æ­£å¸¸æ˜¾ç¤ºï¼‰ ----------------------
st.title("ğŸ“ å¢å¼ºç‰ˆæ–‡æœ¬åˆ†æå·¥å…·ï¼ˆå…¨ä¸­æ–‡æ˜¾ç¤ºç‰ˆï¼‰")
st.divider()

DEFAULT_TEXT = """
ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªšï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥ã€é‡é¤æˆ–è€…éª‘è¡Œï¼Œäº«å—ç¾å¥½çš„å‘¨æœ«æ—¶å…‰ã€‚
å…¬å›­é‡Œçš„èŠ±å¼€å¾—ç‰¹åˆ«æ¼‚äº®ï¼Œæœ‰æ¡ƒèŠ±ã€æ¨±èŠ±ã€éƒé‡‘é¦™ï¼Œäº”é¢œå…­è‰²çš„ï¼Œè®©äººå¿ƒæƒ…æ„‰æ‚¦ã€‚
å’Œå®¶äººä¸€èµ·å‡ºé—¨æ¸¸ç©ï¼ŒèŠèŠå®¶å¸¸ï¼Œåƒåƒç¾é£Ÿï¼Œè¿™æ ·çš„å‘¨æœ«å¤ªå¹¸ç¦äº†ã€‚
å·¥ä½œä¸­é‡åˆ°äº†ä¸€äº›æŒ‘æˆ˜ï¼Œä¸è¿‡åœ¨åŒäº‹çš„å¸®åŠ©ä¸‹ï¼Œç»ˆäºé¡ºåˆ©å®Œæˆäº†é¡¹ç›®ä»»åŠ¡ï¼Œæ”¶è·æ»¡æ»¡ã€‚
å­¦ä¹ ç¼–ç¨‹è™½ç„¶æœ‰ç‚¹éš¾ï¼Œä½†åšæŒä¸‹æ¥å°±èƒ½æŒæ¡å¾ˆå¤šæŠ€èƒ½ï¼Œå¯¹æœªæ¥çš„èŒä¸šå‘å±•å¾ˆæœ‰å¸®åŠ©ã€‚
"""

user_input = st.text_area(
    "è¯·è¾“å…¥å¾…åˆ†ææ–‡æœ¬ï¼ˆå¯ç›´æ¥ä½¿ç”¨ç¤ºä¾‹æ–‡æœ¬æµ‹è¯•ï¼‰",
    height=200,
    placeholder=DEFAULT_TEXT,
    value=DEFAULT_TEXT
)

top_n = st.slider("é€‰æ‹©é«˜é¢‘å…³é”®è¯å±•ç¤ºæ•°é‡", min_value=5, max_value=20, value=10, step=1)
st.divider()

if st.button("ğŸš€ å¼€å§‹åˆ†æ", use_container_width=True):
    if not user_input.strip():
        st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬")
    else:
        text_stats = calculate_text_stats(user_input)
        top_keywords = get_top_keywords(text_stats["çº¯æ–‡æœ¬å†…å®¹"], top_n=top_n)
        sentiment_result = analyze_sentiment(text_stats["çº¯æ–‡æœ¬å†…å®¹"])
        word_segmentation = get_word_segmentation(text_stats["çº¯æ–‡æœ¬å†…å®¹"])

        st.success("âœ… åˆ†æå®Œæˆ")
        st.divider()

        # 1. åŸºç¡€ç»Ÿè®¡ï¼ˆä¸­æ–‡æ¸…æ™°æ˜¾ç¤ºï¼‰
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ“Š åŸºç¡€æ–‡æœ¬ç»Ÿè®¡")
            st.write(f"å«ç©ºæ ¼æ¢è¡Œæ€»å­—ç¬¦æ•°ï¼š{text_stats['å«ç©ºæ ¼æ¢è¡Œæ€»å­—ç¬¦æ•°']}")
            st.write(f"æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°ï¼š{text_stats['æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°']}")
            st.write(f"çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰ï¼š{text_stats['çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰']}")
        with col2:
            st.subheader("ğŸ“‹ æ‰©å±•ç»Ÿè®¡ä¿¡æ¯")
            st.write(f"å¥å­æ•°ï¼š{text_stats['å¥å­æ•°']}")
            st.write(f"æ ‡ç‚¹ç¬¦å·æ•°ï¼š{text_stats['æ ‡ç‚¹ç¬¦å·æ•°']}")
            st.write(f"å¹³å‡æ¯å¥å­—ç¬¦æ•°ï¼š{round(text_stats['æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°']/text_stats['å¥å­æ•°'], 2)}")

        st.divider()

        # 2. é«˜é¢‘å…³é”®è¯ + StreamlitåŸç”ŸæŸ±çŠ¶å›¾ï¼ˆä¸­æ–‡å®Œç¾æ˜¾ç¤ºï¼‰
        st.subheader(f"ğŸ”¤ é«˜é¢‘å…³é”®è¯TOP{top_n}")
        if top_keywords:
            keyword_dict = {"å…³é”®è¯": [item[0] for item in top_keywords], "å‡ºç°æ¬¡æ•°": [item[1] for item in top_keywords]}
            st.table(keyword_dict)
            
            # åŸç”ŸæŸ±çŠ¶å›¾ï¼ˆä¸­æ–‡æ— å‹åŠ›ï¼‰
            st.subheader("ğŸ“Š é«˜é¢‘å…³é”®è¯æŸ±çŠ¶å›¾")
            st.bar_chart(
                data=keyword_dict,
                x="å…³é”®è¯",
                y="å‡ºç°æ¬¡æ•°",
                color="#2E86AB",
                use_container_width=True
            )
        else:
            st.info("ğŸ“Œ æ— æœ‰æ•ˆå…³é”®è¯ï¼ˆæœªç­›é€‰å‡ºé•¿åº¦>1ä¸”éåœç”¨è¯çš„è¯æ±‡ï¼‰")

        st.divider()

        # 3. ä¸­æ–‡åˆ†è¯ç»“æœï¼ˆä¸­æ–‡æ¸…æ™°æ˜¾ç¤ºï¼‰
        st.subheader("âœ‚ï¸ ä¸­æ–‡åˆ†è¯ç»“æœ")
        st.text_area("åˆ†è¯ç»“æœï¼ˆ| åˆ†éš”ï¼‰", value=word_segmentation, height=100, disabled=True)

        st.divider()

        # 4. æƒ…æ„Ÿåˆ†æ + åŸç”Ÿå‚è€ƒå±•ç¤ºï¼ˆæ›¿ä»£Matplotlibå›¾ï¼Œä¸­æ–‡æ­£å¸¸ï¼‰
        st.subheader("â¤ï¸ æƒ…æ„Ÿå€¾å‘åˆ†æ")
        col3, col4 = st.columns(2)
        with col3:
            st.write(f"æƒ…æ„Ÿå¾—åˆ†ï¼š{sentiment_result['æƒ…æ„Ÿå¾—åˆ†']}ï¼ˆ0=è´Ÿé¢ï¼Œ1=æ­£é¢ï¼‰")
            st.write(f"æƒ…æ„Ÿå€¾å‘ï¼š{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
            if sentiment_result['æƒ…æ„Ÿå€¾å‘'] == "æ­£é¢":
                st.success(f"âœ… æ–‡æœ¬æ•´ä½“åå‘{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
            elif sentiment_result['æƒ…æ„Ÿå€¾å‘'] == "è´Ÿé¢":
                st.error(f"âŒ æ–‡æœ¬æ•´ä½“åå‘{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
            else:
                st.info(f"â„¹ï¸ æ–‡æœ¬æ•´ä½“ä¸º{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
        with col4:
            st.subheader("ğŸ“ æ–‡æœ¬è‡ªåŠ¨æ‘˜è¦")
            summary_list = sentiment_result['æ–‡æœ¬æ‘˜è¦']
            if summary_list:
                for idx, summary in enumerate(summary_list, 1):
                    st.write(f"{idx}. {summary}")
            else:
                st.info("ğŸ“Œ æ— æ³•ç”Ÿæˆæœ‰æ•ˆæ‘˜è¦")
        
        # åŸç”Ÿæƒ…æ„Ÿå‚è€ƒå±•ç¤º
        show_sentiment_reference(sentiment_result["æƒ…æ„Ÿå¾—åˆ†"])

        st.divider()

        # 5. æ–‡æœ¬æ„æˆå±•ç¤ºï¼ˆæ›¿ä»£Matplotlibé¥¼å›¾ï¼Œä¸­æ–‡æ­£å¸¸ï¼‰
        st.subheader("ğŸ¥§ æ–‡æœ¬æ„æˆå æ¯”")
        show_text_composition(text_stats)

        st.divider()

        # 6. å…³é”®è¯æƒé‡å±•ç¤ºï¼ˆæ›¿ä»£è¯äº‘å›¾ï¼Œä¸­æ–‡æ¸…æ™°æ˜¾ç¤ºï¼‰
        show_wordcloud_alternative(text_stats["çº¯æ–‡æœ¬å†…å®¹"])

        st.divider()
        st.caption("ğŸ’¡ å…¨æ¨¡å—é‡‡ç”¨StreamlitåŸç”Ÿç»„ä»¶ï¼Œå½»åº•è§£å†³ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ï¼Œæ— å­—ä½“ä¾èµ–")