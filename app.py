import streamlit as st
import jieba
from collections import Counter
import numpy as np
from snownlp import SnowNLP
import pandas as pd
import requests
from bs4 import BeautifulSoup
import re

# é¡µé¢é…ç½®
st.set_page_config(page_title="URL+æ–‡æœ¬åŒæ¨¡å¼åˆ†æå·¥å…·", page_icon="ğŸ“", layout="centered")

# æ‰©å……åœç”¨è¯è¡¨
STOP_WORDS = {
    "çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ",
    "åœ¨", "å’Œ", "æœ‰", "å°±", "éƒ½", "è¿™", "é‚£", "å…¶",
    "ä¹‹", "äº", "ä»¥", "ä¸º", "è€Œ", "ä¹Ÿ", "å—", "å‘¢",
    "å§", "å•Š", "å“¦", "å—¯", "ç€", "è¿‡", "è¿˜", "å°†",
    "è¦", "ä¼š", "èƒ½", "å¯", "å¯¹", "ä¸", "æˆ–", "åŠ",
    "æ‰€", "æŠŠ", "è¢«", "è®©", "ç»™", "ä½¿", "å¾—", "åˆ°",
    "ä»", "å¾€", "å‘", "æ¯”", "è·Ÿ", "åŒ", "å’Œ"
}

# ---------------------- æ–°å¢ï¼šç½‘é¡µURLæ–‡æœ¬çˆ¬å–å‡½æ•° ----------------------
def crawl_webpage_text(url):
    """
    çˆ¬å–æŒ‡å®šURLçš„ç½‘é¡µæ­£æ–‡æ–‡æœ¬ï¼Œå»é™¤HTMLæ ‡ç­¾ã€å¤šä½™ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
    """
    try:
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨è®¿é—®ï¼ˆé¿å…è¢«åçˆ¬ï¼‰
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        # å‘é€GETè¯·æ±‚è·å–ç½‘é¡µå†…å®¹
        response = requests.get(url, headers=headers, timeout=15)
        response.raise_for_status()  # è‹¥è¯·æ±‚å¤±è´¥ï¼ˆ4xx/5xxï¼‰ï¼ŒæŠ›å‡ºå¼‚å¸¸
        response.encoding = response.apparent_encoding  # è‡ªåŠ¨è¯†åˆ«ç¼–ç ï¼Œé¿å…ä¹±ç 

        # ä½¿ç”¨BeautifulSoupè§£æHTMLï¼Œæå–æ­£æ–‡
        soup = BeautifulSoup(response.text, "html.parser")

        # ç§»é™¤scriptã€styleæ ‡ç­¾ï¼ˆæ— å…³å†…å®¹ï¼‰
        for script in soup(["script", "style"]):
            script.decompose()

        # æå–æ–‡æœ¬å†…å®¹ï¼Œå»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
        raw_text = soup.get_text()
        # æ¸…ç†æ–‡æœ¬ï¼šå»é™¤å¤šä¸ªç©ºæ ¼ã€æ¢è¡Œã€åˆ¶è¡¨ç¬¦
        clean_text = re.sub(r'\s+', ' ', raw_text).strip()

        if not clean_text:
            return None, "æœªä»è¯¥URLä¸­æå–åˆ°æœ‰æ•ˆæ–‡æœ¬"
        return clean_text, "çˆ¬å–æˆåŠŸ"

    except requests.exceptions.Timeout:
        return None, "è¯·æ±‚è¶…æ—¶ï¼ˆè¯·æ£€æŸ¥URLæ˜¯å¦æœ‰æ•ˆæˆ–ç½‘ç»œçŠ¶å†µï¼‰"
    except requests.exceptions.HTTPError as e:
        return None, f"ç½‘é¡µè¯·æ±‚å¤±è´¥ï¼š{e}ï¼ˆHTTPçŠ¶æ€ç å¼‚å¸¸ï¼‰"
    except requests.exceptions.RequestException as e:
        return None, f"çˆ¬å–å¤±è´¥ï¼š{e}ï¼ˆURLæ— æ•ˆæˆ–ç½‘ç»œå¼‚å¸¸ï¼‰"
    except Exception as e:
        return None, f"æœªçŸ¥é”™è¯¯ï¼š{e}"

# ---------------------- æ ¸å¿ƒæ–‡æœ¬åˆ†æå‡½æ•° ----------------------
def calculate_text_stats(input_text):
    total_with_space = len(input_text)
    pure_text = input_text.replace(" ", "").replace("\n", "")
    total_without_space = len(pure_text)
    
    sentence_end_chars = "ã€‚ï¼ï¼Ÿï¼›"
    sentence_count = 1
    for char in sentence_end_chars:
        sentence_count += pure_text.count(char)
    
    punctuation_chars = 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹,.!?;:\'"()[]{}<>ã€'
    punctuation_count = sum(1 for char in pure_text if char in punctuation_chars)
    pure_word_count = total_without_space - punctuation_count
    
    return {
        "å«ç©ºæ ¼æ¢è¡Œæ€»å­—ç¬¦æ•°": total_with_space,
        "æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°": total_without_space,
        "çº¯æ–‡æœ¬å†…å®¹": pure_text,
        "å¥å­æ•°": sentence_count,
        "æ ‡ç‚¹ç¬¦å·æ•°": punctuation_count,
        "çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰": pure_word_count
    }

def get_top_keywords(pure_text, top_n=10):
    if not pure_text:
        return []
    word_list = jieba.lcut(pure_text)
    valid_words = [
        word for word in word_list
        if word not in STOP_WORDS and len(word) > 1 and word.strip()
    ]
    if not valid_words:
        return []
    word_count = Counter(valid_words)
    return word_count.most_common(top_n)

def analyze_sentiment(pure_text):
    if not pure_text:
        return {"æƒ…æ„Ÿå¾—åˆ†": 0.5, "æƒ…æ„Ÿå€¾å‘": "ä¸­æ€§", "æ–‡æœ¬æ‘˜è¦": []}
    s = SnowNLP(pure_text)
    sentiment_score = s.sentiments
    if sentiment_score >= 0.7:
        sentiment_tendency = "æ­£é¢"
    elif sentiment_score <= 0.3:
        sentiment_tendency = "è´Ÿé¢"
    else:
        sentiment_tendency = "ä¸­æ€§"
    summary_list = s.summary(3) if len(pure_text) > 10 else ["æ–‡æœ¬è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦"]
    return {
        "æƒ…æ„Ÿå¾—åˆ†": round(sentiment_score, 4),
        "æƒ…æ„Ÿå€¾å‘": sentiment_tendency,
        "æ–‡æœ¬æ‘˜è¦": summary_list
    }

def get_word_segmentation(pure_text):
    if not pure_text:
        return "æ— æœ‰æ•ˆæ–‡æœ¬"
    word_list = jieba.lcut(pure_text)
    filtered_word_list = [word for word in word_list if word not in STOP_WORDS and word.strip()]
    if not filtered_word_list:
        return "æ— æœ‰æ•ˆåˆ†è¯ï¼ˆå…¨ä¸ºåœç”¨è¯/æ ‡ç‚¹ï¼‰"
    return " | ".join(filtered_word_list)

def show_text_composition(text_stats):
    pure_word_count = text_stats["çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰"]
    punctuation_count = text_stats["æ ‡ç‚¹ç¬¦å·æ•°"]
    total = pure_word_count + punctuation_count
    
    if total == 0:
        st.info("ğŸ“Œ æ— æœ‰æ•ˆæ–‡æœ¬æ•°æ®å¯å±•ç¤º")
        return
    
    word_ratio = round((pure_word_count / total) * 100, 1)
    punctuation_ratio = round((punctuation_count / total) * 100, 1)
    
    comp_data = pd.DataFrame({
        "æ–‡æœ¬ç±»å‹": ["çº¯æ–‡å­—", "æ ‡ç‚¹ç¬¦å·"],
        "æ•°é‡": [pure_word_count, punctuation_count],
        "å æ¯”(%)": [word_ratio, punctuation_ratio]
    })
    st.table(comp_data)
    
    st.write("### å æ¯”å¯è§†åŒ–")
    col1, col2 = st.columns(2)
    with col1:
        st.write(f"çº¯æ–‡å­—ï¼ˆ{word_ratio}%ï¼‰")
        st.progress(word_ratio / 100)
    with col2:
        st.write(f"æ ‡ç‚¹ç¬¦å·ï¼ˆ{punctuation_ratio}%ï¼‰")
        st.progress(punctuation_ratio / 100)

def show_sentiment_reference(sentiment_score):
    st.write("### æƒ…æ„Ÿå¾—åˆ†åŒºé—´è¯´æ˜")
    st.markdown("""
    | å¾—åˆ†åŒºé—´ | æƒ…æ„Ÿå€¾å‘ |
    |----------|----------|
    | 0.0 - 0.3 | è´Ÿé¢ |
    | 0.3 - 0.7 | ä¸­æ€§ |
    | 0.7 - 1.0 | æ­£é¢ |
    """)
    
    sentiment_label = "æ­£é¢" if sentiment_score >=0.7 else "è´Ÿé¢" if sentiment_score <=0.3 else "ä¸­æ€§"
    st.write(f"#### å½“å‰æ–‡æœ¬ï¼š{sentiment_label}ï¼ˆå¾—åˆ†ï¼š{sentiment_score}ï¼‰")
    
    if sentiment_label == "æ­£é¢":
        st.success(f"âœ… æƒ…æ„Ÿå€¾å‘ï¼š{sentiment_label}")
    elif sentiment_label == "è´Ÿé¢":
        st.error(f"âŒ æƒ…æ„Ÿå€¾å‘ï¼š{sentiment_label}")
    else:
        st.info(f"â„¹ï¸ æƒ…æ„Ÿå€¾å‘ï¼š{sentiment_label}")

def show_wordcloud_alternative(pure_text):
    st.subheader("â˜ï¸ å…³é”®è¯æƒé‡å±•ç¤ºï¼ˆæ›¿ä»£è¯äº‘å›¾ï¼Œä¸­æ–‡æ¸…æ™°æ˜¾ç¤ºï¼‰")
    top_keywords = get_top_keywords(pure_text, top_n=20)
    if not top_keywords:
        st.info("ğŸ“Œ æ— æœ‰æ•ˆå…³é”®è¯å¯å±•ç¤º")
        return
    
    for word, count in top_keywords:
        font_size = min(12 + count * 2, 20)
        st.markdown(f"<span style='font-size:{font_size}px; color:#2E86AB; font-weight:bold;'>{word}</span> ï¼ˆå‡ºç°{count}æ¬¡ï¼‰", unsafe_allow_html=True)

# ---------------------- é¡µé¢äº¤äº’ï¼ˆå«URLçˆ¬å–+æ‰‹åŠ¨è¾“å…¥åŒæ¨¡å¼ï¼‰ ----------------------
st.title("ğŸ“ URL+æ‰‹åŠ¨è¾“å…¥ åŒæ¨¡å¼æ–‡æœ¬åˆ†æå·¥å…·")
st.divider()

# é€‰æ‹©åˆ†ææ¨¡å¼
analysis_mode = st.radio("è¯·é€‰æ‹©åˆ†ææ¨¡å¼", ("ç½‘é¡µURLçˆ¬å–åˆ†æ", "æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬åˆ†æ"), horizontal=True)

input_text = ""
crawl_status = ""

# æ¨¡å¼1ï¼šç½‘é¡µURLçˆ¬å–åˆ†æ
if analysis_mode == "ç½‘é¡µURLçˆ¬å–åˆ†æ":
    st.subheader("ğŸ”— ç½‘é¡µURLè¾“å…¥")
    web_url = st.text_input("è¯·è¾“å…¥æœ‰æ•ˆç½‘é¡µURLï¼ˆç¤ºä¾‹ï¼šhttps://www.xxx.com/articleï¼‰", placeholder="https://...")
    
    # çˆ¬å–æŒ‰é’®
    if st.button("ğŸŒ å¼€å§‹çˆ¬å–ç½‘é¡µæ–‡æœ¬", use_container_width=True):
        if not web_url.strip():
            st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆçš„URLåœ°å€")
        else:
            with st.spinner("æ­£åœ¨çˆ¬å–ç½‘é¡µæ–‡æœ¬ï¼Œè¯·ç¨å€™..."):
                crawled_text, msg = crawl_webpage_text(web_url)
                if crawled_text:
                    crawl_status = msg
                    input_text = crawled_text
                    st.success(f"âœ… {msg}ï¼å·²æå–åˆ°æ–‡æœ¬ï¼Œå¯è¿›è¡Œåˆ†æ")
                    # å±•ç¤ºçˆ¬å–çš„æ–‡æœ¬ï¼ˆæŠ˜å é¢æ¿ï¼Œé¿å…å ç”¨è¿‡å¤šç©ºé—´ï¼‰
                    with st.expander("æŸ¥çœ‹çˆ¬å–çš„åŸå§‹æ–‡æœ¬", expanded=False):
                        st.text_area("çˆ¬å–æ–‡æœ¬", value=input_text, height=150, disabled=True)
                else:
                    st.error(f"âŒ {msg}")

# æ¨¡å¼2ï¼šæ‰‹åŠ¨è¾“å…¥æ–‡æœ¬åˆ†æ
else:
    st.subheader("âœï¸ æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬")
    DEFAULT_TEXT = """
ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªšï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥ã€é‡é¤æˆ–è€…éª‘è¡Œï¼Œäº«å—ç¾å¥½çš„å‘¨æœ«æ—¶å…‰ã€‚
å…¬å›­é‡Œçš„èŠ±å¼€å¾—ç‰¹åˆ«æ¼‚äº®ï¼Œæœ‰æ¡ƒèŠ±ã€æ¨±èŠ±ã€éƒé‡‘é¦™ï¼Œäº”é¢œå…­è‰²çš„ï¼Œè®©äººå¿ƒæƒ…æ„‰æ‚¦ã€‚
å’Œå®¶äººä¸€èµ·å‡ºé—¨æ¸¸ç©ï¼ŒèŠèŠå®¶å¸¸ï¼Œåƒåƒç¾é£Ÿï¼Œè¿™æ ·çš„å‘¨æœ«å¤ªå¹¸ç¦äº†ã€‚
å·¥ä½œä¸­é‡åˆ°äº†ä¸€äº›æŒ‘æˆ˜ï¼Œä¸è¿‡åœ¨åŒäº‹çš„å¸®åŠ©ä¸‹ï¼Œç»ˆäºé¡ºåˆ©å®Œæˆäº†é¡¹ç›®ä»»åŠ¡ï¼Œæ”¶è·æ»¡æ»¡ã€‚
å­¦ä¹ ç¼–ç¨‹è™½ç„¶æœ‰ç‚¹éš¾ï¼Œä½†åšæŒä¸‹æ¥å°±èƒ½æŒæ¡å¾ˆå¤šæŠ€èƒ½ï¼Œå¯¹æœªæ¥çš„èŒä¸šå‘å±•å¾ˆæœ‰å¸®åŠ©ã€‚
    """
    input_text = st.text_area(
        "è¯·è¾“å…¥å¾…åˆ†ææ–‡æœ¬",
        height=200,
        placeholder=DEFAULT_TEXT,
        value=DEFAULT_TEXT
    )

# é€šç”¨åˆ†æé…ç½®
top_n = st.slider("é€‰æ‹©é«˜é¢‘å…³é”®è¯å±•ç¤ºæ•°é‡", min_value=5, max_value=20, value=10, step=1)
st.divider()

# å¼€å§‹åˆ†ææŒ‰é’®ï¼ˆé€šç”¨ï¼‰
if st.button("ğŸš€ å¼€å§‹æ–‡æœ¬åˆ†æ", use_container_width=True):
    if not input_text.strip():
        st.warning("âš ï¸ æ— æœ‰æ•ˆæ–‡æœ¬å¯åˆ†æï¼ˆè¯·å…ˆçˆ¬å–ç½‘é¡µæ–‡æœ¬æˆ–æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬ï¼‰")
    else:
        # æ ¸å¿ƒåˆ†ææµç¨‹
        text_stats = calculate_text_stats(input_text)
        top_keywords = get_top_keywords(text_stats["çº¯æ–‡æœ¬å†…å®¹"], top_n=top_n)
        sentiment_result = analyze_sentiment(text_stats["çº¯æ–‡æœ¬å†…å®¹"])
        word_segmentation = get_word_segmentation(text_stats["çº¯æ–‡æœ¬å†…å®¹"])

        st.success("âœ… æ–‡æœ¬åˆ†æå®Œæˆï¼")
        st.divider()

        # 1. åŸºç¡€æ–‡æœ¬ç»Ÿè®¡
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ“Š åŸºç¡€æ–‡æœ¬ç»Ÿè®¡")
            st.write(f"å«ç©ºæ ¼æ¢è¡Œæ€»å­—ç¬¦æ•°ï¼š{text_stats['å«ç©ºæ ¼æ¢è¡Œæ€»å­—ç¬¦æ•°']}")
            st.write(f"æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°ï¼š{text_stats['æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°']}")
            st.write(f"çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰ï¼š{text_stats['çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰']}")
        with col2:
            st.subheader("ğŸ“‹ æ‰©å±•ç»Ÿè®¡ä¿¡æ¯")
            st.write(f"å¥å­æ•°ï¼š{text_stats['å¥å­æ•°']}")
            st.write(f"æ ‡ç‚¹ç¬¦å·æ•°ï¼š{text_stats['æ ‡ç‚¹ç¬¦å·æ•°']}")
            st.write(f"å¹³å‡æ¯å¥å­—ç¬¦æ•°ï¼š{round(text_stats['æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°']/text_stats['å¥å­æ•°'], 2)}")

        st.divider()

        # 2. é«˜é¢‘å…³é”®è¯ + StreamlitåŸç”ŸæŸ±çŠ¶å›¾
        st.subheader(f"ğŸ”¤ é«˜é¢‘å…³é”®è¯TOP{top_n}")
        if top_keywords:
            keyword_dict = {"å…³é”®è¯": [item[0] for item in top_keywords], "å‡ºç°æ¬¡æ•°": [item[1] for item in top_keywords]}
            st.table(keyword_dict)
            
            st.subheader("ğŸ“Š é«˜é¢‘å…³é”®è¯æŸ±çŠ¶å›¾")
            st.bar_chart(
                data=keyword_dict,
                x="å…³é”®è¯",
                y="å‡ºç°æ¬¡æ•°",
                color="#2E86AB",
                use_container_width=True
            )
        else:
            st.info("ğŸ“Œ æ— æœ‰æ•ˆå…³é”®è¯ï¼ˆæœªç­›é€‰å‡ºé•¿åº¦>1ä¸”éåœç”¨è¯çš„è¯æ±‡ï¼‰")

        st.divider()

        # 3. ä¸­æ–‡åˆ†è¯ç»“æœ
        st.subheader("âœ‚ï¸ ä¸­æ–‡åˆ†è¯ç»“æœ")
        st.text_area("åˆ†è¯ç»“æœï¼ˆ| åˆ†éš”ï¼‰", value=word_segmentation, height=100, disabled=True)

        st.divider()

        # 4. æƒ…æ„Ÿåˆ†æ + å‚è€ƒå±•ç¤º
        st.subheader("â¤ï¸ æƒ…æ„Ÿå€¾å‘åˆ†æ")
        col3, col4 = st.columns(2)
        with col3:
            st.write(f"æƒ…æ„Ÿå¾—åˆ†ï¼š{sentiment_result['æƒ…æ„Ÿå¾—åˆ†']}ï¼ˆ0=è´Ÿé¢ï¼Œ1=æ­£é¢ï¼‰")
            st.write(f"æƒ…æ„Ÿå€¾å‘ï¼š{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
            if sentiment_result['æƒ…æ„Ÿå€¾å‘'] == "æ­£é¢":
                st.success(f"âœ… æ–‡æœ¬æ•´ä½“åå‘{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
            elif sentiment_result['æƒ…æ„Ÿå€¾å‘'] == "è´Ÿé¢":
                st.error(f"âŒ æ–‡æœ¬æ•´ä½“åå‘{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
            else:
                st.info(f"â„¹ï¸ æ–‡æœ¬æ•´ä½“ä¸º{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
        with col4:
            st.subheader("ğŸ“ æ–‡æœ¬è‡ªåŠ¨æ‘˜è¦")
            summary_list = sentiment_result['æ–‡æœ¬æ‘˜è¦']
            if summary_list:
                for idx, summary in enumerate(summary_list, 1):
                    st.write(f"{idx}. {summary}")
            else:
                st.info("ğŸ“Œ æ— æ³•ç”Ÿæˆæœ‰æ•ˆæ‘˜è¦")
        
        show_sentiment_reference(sentiment_result["æƒ…æ„Ÿå¾—åˆ†"])

        st.divider()

        # 5. æ–‡æœ¬æ„æˆå æ¯”
        st.subheader("ğŸ¥§ æ–‡æœ¬æ„æˆå æ¯”")
        show_text_composition(text_stats)

        st.divider()

        # 6. å…³é”®è¯æƒé‡å±•ç¤º
        show_wordcloud_alternative(text_stats["çº¯æ–‡æœ¬å†…å®¹"])

        st.divider()
        st.caption("ğŸ’¡ æ”¯æŒURLçˆ¬å–å’Œæ‰‹åŠ¨è¾“å…¥åŒæ¨¡å¼ï¼Œå…¨æ¨¡å—ä¸­æ–‡æ­£å¸¸æ˜¾ç¤ºï¼Œæ— å­—ä½“ä¾èµ–")