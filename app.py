import streamlit as st
import jieba
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import numpy as np
from snownlp import SnowNLP

# é¡µé¢é…ç½®
st.set_page_config(page_title="å¢å¼ºç‰ˆæ–‡æœ¬åˆ†æå·¥å…·", page_icon="ğŸ“", layout="centered")

# æ‰©å……åœç”¨è¯è¡¨ï¼ˆæ›´å…¨é¢çš„ä¸­æ–‡åœç”¨è¯ï¼‰
STOP_WORDS = {
    "çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ",
    "åœ¨", "å’Œ", "æœ‰", "å°±", "éƒ½", "è¿™", "é‚£", "å…¶",
    "ä¹‹", "äº", "ä»¥", "ä¸º", "è€Œ", "ä¹Ÿ", "å—", "å‘¢",
    "å§", "å•Š", "å“¦", "å—¯", "ç€", "è¿‡", "è¿˜", "å°†",
    "è¦", "ä¼š", "èƒ½", "å¯", "å¯¹", "ä¸", "æˆ–", "åŠ",
    "æ‰€", "æŠŠ", "è¢«", "è®©", "ç»™", "ä½¿", "å¾—", "åˆ°",
    "ä»", "å¾€", "å‘", "æ¯”", "è·Ÿ", "åŒ", "å’Œ", "çš„"
}

# ---------------------- æ ¸å¿ƒå‡½æ•°ï¼ˆæ‰©å……åŠŸèƒ½ï¼Œæ–°å¢å›¾å½¢ç›¸å…³å‡½æ•°ï¼‰ ----------------------
def calculate_text_stats(input_text):
    total_with_space = len(input_text)
    pure_text = input_text.replace(" ", "").replace("\n", "")
    total_without_space = len(pure_text)
    
    # æ–°å¢ç»Ÿè®¡é¡¹ï¼šå¥å­æ•°ï¼ˆæŒ‰ã€‚ï¼ï¼Ÿåˆ†å‰²ï¼‰
    sentence_end_chars = "ã€‚ï¼ï¼Ÿï¼›"
    sentence_count = 1  # é»˜è®¤è‡³å°‘1ä¸ªå¥å­
    for char in sentence_end_chars:
        sentence_count += pure_text.count(char)
    
    # æ–°å¢ç»Ÿè®¡é¡¹ï¼šæ ‡ç‚¹ç¬¦å·æ•°
    punctuation_chars = 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹,.!?;:\'"()[]{}<>ã€'
    punctuation_count = sum(1 for char in pure_text if char in punctuation_chars)
    
    # æ–°å¢ç»Ÿè®¡é¡¹ï¼šçº¯æ–‡å­—æ•°ï¼ˆå»é™¤æ ‡ç‚¹ï¼‰
    pure_word_count = total_without_space - punctuation_count
    
    return {
        "å«ç©ºæ ¼æ¢è¡Œæ€»å­—ç¬¦æ•°": total_with_space,
        "æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°": total_without_space,
        "çº¯æ–‡æœ¬å†…å®¹": pure_text,
        "å¥å­æ•°": sentence_count,
        "æ ‡ç‚¹ç¬¦å·æ•°": punctuation_count,
        "çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰": pure_word_count
    }

def get_top_keywords(pure_text, top_n=10):
    """è¿”å›åˆ—è¡¨ï¼Œè€ŒéDataFrameï¼Œå½»åº•è§„é¿ç±»å‹é”™è¯¯"""
    word_list = jieba.lcut(pure_text)
    valid_words = [
        word for word in word_list
        if word not in STOP_WORDS and len(word) > 1
    ]
    if not valid_words:
        return []
    word_count = Counter(valid_words)
    return word_count.most_common(top_n)

def generate_wordcloud(pure_text):
    """ç”Ÿæˆä¸­æ–‡è¯äº‘å›¾"""
    # åˆ†è¯å¹¶è¿‡æ»¤åœç”¨è¯
    word_list = jieba.lcut(pure_text)
    valid_words = " ".join([word for word in word_list if word not in STOP_WORDS and len(word) > 1])
    
    if not valid_words:
        return None
    
    # è®¾ç½®è¯äº‘å‚æ•°ï¼ˆæ”¯æŒä¸­æ–‡æ˜¾ç¤ºï¼‰
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'WenQuanYi Zen Hei']  # å…¼å®¹ä¸åŒç¯å¢ƒå­—ä½“
    wc = WordCloud(
        width=800,
        height=400,
        background_color="white",
        font_path=None,  # è‡ªåŠ¨é€‚é…ç³»ç»Ÿä¸­æ–‡å­—ä½“
        max_words=100,
        max_font_size=100,
        random_state=42
    ).generate(valid_words)
    
    # è½¬æ¢ä¸ºå›¾ç‰‡æ ¼å¼ä¾›Streamlitå±•ç¤º
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")  # éšè—åæ ‡è½´
    plt.tight_layout()
    return fig

def analyze_sentiment(pure_text):
    """æ–‡æœ¬æƒ…æ„Ÿå€¾å‘åˆ†æï¼ˆåŸºäºSnowNLPï¼‰"""
    if not pure_text:
        return {"æƒ…æ„Ÿå¾—åˆ†": 0.5, "æƒ…æ„Ÿå€¾å‘": "ä¸­æ€§", "æ–‡æœ¬æ‘˜è¦": []}
    
    s = SnowNLP(pure_text)
    sentiment_score = s.sentiments  # å¾—åˆ†èŒƒå›´0-1ï¼Œè¶Šæ¥è¿‘1è¶Šæ­£é¢ï¼Œè¶Šæ¥è¿‘0è¶Šè´Ÿé¢
    
    # åˆ¤æ–­æƒ…æ„Ÿå€¾å‘
    if sentiment_score >= 0.7:
        sentiment_tendency = "æ­£é¢"
    elif sentiment_score <= 0.3:
        sentiment_tendency = "è´Ÿé¢"
    else:
        sentiment_tendency = "ä¸­æ€§"
    
    return {
        "æƒ…æ„Ÿå¾—åˆ†": round(sentiment_score, 4),
        "æƒ…æ„Ÿå€¾å‘": sentiment_tendency,
        "æ–‡æœ¬æ‘˜è¦": s.summary(3)  # ç”Ÿæˆ3å¥æ–‡æœ¬æ‘˜è¦
    }

def get_word_segmentation(pure_text):
    """è¿”å›ä¸­æ–‡åˆ†è¯ç»“æœï¼ˆå¸¦åˆ†éš”ç¬¦ï¼‰"""
    word_list = jieba.lcut(pure_text)
    # è¿‡æ»¤åœç”¨è¯ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹åˆ†è¯å±•ç¤º
    filtered_word_list = [word for word in word_list if word not in STOP_WORDS]
    return " | ".join(filtered_word_list)

def plot_keyword_bar(top_keywords):
    """ç»˜åˆ¶é«˜é¢‘å…³é”®è¯æŸ±çŠ¶å›¾ï¼ˆå±•ç¤ºå…³é”®è¯ä¸å‡ºç°æ¬¡æ•°ï¼‰"""
    if not top_keywords:
        return None
    
    plt.rcParams['font.sans-serif'] = ['SimHei', 'WenQuanYi Zen Hei']  # æ”¯æŒä¸­æ–‡
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
    
    words = [item[0] for item in top_keywords]
    counts = [item[1] for item in top_keywords]
    
    # åˆ›å»ºæŸ±çŠ¶å›¾
    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(words, counts, color='#2E86AB', alpha=0.8, edgecolor='#1A5276')
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{int(height)}', ha='center', va='bottom', fontsize=10)
    
    ax.set_xlabel('é«˜é¢‘å…³é”®è¯', fontsize=12, fontweight='bold')
    ax.set_ylabel('å‡ºç°æ¬¡æ•°', fontsize=12, fontweight='bold')
    ax.set_title('é«˜é¢‘å…³é”®è¯å‡ºç°æ¬¡æ•°æŸ±çŠ¶å›¾', fontsize=14, fontweight='bold', pad=20)
    plt.xticks(rotation=45, ha='right')  # å…³é”®è¯æ¨ªå‘æ—‹è½¬ï¼Œé¿å…é‡å 
    plt.tight_layout()
    return fig

def plot_text_composition_pie(text_stats):
    """ç»˜åˆ¶æ–‡æœ¬æ„æˆé¥¼å›¾ï¼ˆçº¯æ–‡å­— vs æ ‡ç‚¹ç¬¦å·ï¼‰"""
    pure_word_count = text_stats["çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰"]
    punctuation_count = text_stats["æ ‡ç‚¹ç¬¦å·æ•°"]
    
    if pure_word_count + punctuation_count == 0:
        return None
    
    # æ•°æ®ä¸æ ‡ç­¾
    labels = ['çº¯æ–‡å­—', 'æ ‡ç‚¹ç¬¦å·']
    sizes = [pure_word_count, punctuation_count]
    colors = ['#A23B72', '#F18F01']
    explode = (0.05, 0)  # çªå‡ºçº¯æ–‡å­—éƒ¨åˆ†
    
    # åˆ›å»ºé¥¼å›¾
    fig, ax = plt.subplots(figsize=(8, 6))
    wedges, texts, autotexts = ax.pie(sizes, explode=explode, labels=labels, colors=colors,
                                      autopct='%1.1f%%', shadow=True, startangle=90,
                                      textprops={'fontsize': 10})
    
    # ç¾åŒ–ç™¾åˆ†æ¯”æ–‡å­—
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
    
    ax.set_title('æ–‡æœ¬æ„æˆå æ¯”é¥¼å›¾ï¼ˆçº¯æ–‡å­—/æ ‡ç‚¹ç¬¦å·ï¼‰', fontsize=14, fontweight='bold', pad=20)
    plt.tight_layout()
    return fig

def plot_sentiment_reference_line(sentiment_score):
    """ç»˜åˆ¶æƒ…æ„Ÿå¾—åˆ†å‚è€ƒæŠ˜çº¿å›¾ï¼ˆè¾…åŠ©ç›´è§‚åˆ¤æ–­æƒ…æ„Ÿå€¾å‘ï¼‰"""
    plt.rcParams['font.sans-serif'] = ['SimHei', 'WenQuanYi Zen Hei']
    plt.rcParams['axes.unicode_minus'] = False
    
    # ç”Ÿæˆå‚è€ƒæ•°æ®
    x = [0, 0.3, 0.7, 1]
    y = [0, 0, 0, 0]
    labels = ['è´Ÿé¢', 'ä¸­æ€§é˜ˆå€¼', 'æ­£é¢é˜ˆå€¼', 'æ­£é¢']
    
    # åˆ›å»ºå›¾å½¢
    fig, ax = plt.subplots(figsize=(10, 3))
    # ç»˜åˆ¶å‚è€ƒçº¿
    ax.plot(x, y, color='#C73E1D', linewidth=2, linestyle='--', label='æƒ…æ„Ÿå€¾å‘åˆ†ç•Œçº¿')
    # ç»˜åˆ¶å½“å‰æƒ…æ„Ÿå¾—åˆ†ç‚¹
    ax.scatter(sentiment_score, 0, color='#2E86AB', s=200, zorder=5, label=f'å½“å‰å¾—åˆ†ï¼š{sentiment_score}')
    
    # æ·»åŠ æ ‡æ³¨
    for i, label in enumerate(labels):
        ax.text(x[i], 0.05, label, ha='center', va='bottom', fontsize=10, fontweight='bold')
    # æ ‡æ³¨æƒ…æ„Ÿå€¾å‘
    sentiment_label = "æ­£é¢" if sentiment_score >=0.7 else "è´Ÿé¢" if sentiment_score <=0.3 else "ä¸­æ€§"
    ax.text(sentiment_score, -0.05, sentiment_label, ha='center', va='top', 
            fontsize=11, fontweight='bold', color='red')
    
    ax.set_xlim(-0.1, 1.1)
    ax.set_ylim(-0.1, 0.1)
    ax.set_xlabel('æƒ…æ„Ÿå¾—åˆ†åŒºé—´', fontsize=12, fontweight='bold')
    ax.set_title('æƒ…æ„Ÿå¾—åˆ†å‚è€ƒå›¾ï¼ˆ0=è´Ÿé¢ï¼Œ1=æ­£é¢ï¼‰', fontsize=14, fontweight='bold', pad=20)
    ax.legend(loc='upper right')
    ax.axis('off')  # éšè—åæ ‡è½´ï¼Œä»…å±•ç¤ºå‚è€ƒä¿¡æ¯
    plt.tight_layout()
    return fig

# ---------------------- é¡µé¢äº¤äº’ï¼ˆæ‰©å……å±•ç¤ºï¼Œæ–°å¢å›¾å½¢å±•ç¤ºæ¨¡å—ï¼‰ ----------------------
st.title("ğŸ“ å¢å¼ºç‰ˆæ–‡æœ¬åˆ†æWebåº”ç”¨ï¼ˆå«å›¾å½¢å¯è§†åŒ–ï¼‰")
st.divider()

user_input = st.text_area(
    "è¯·è¾“å…¥å¾…åˆ†ææ–‡æœ¬",
    height=200,
    placeholder="ç¤ºä¾‹ï¼šä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªšï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥ã€é‡é¤æˆ–è€…éª‘è¡Œï¼Œäº«å—ç¾å¥½çš„å‘¨æœ«æ—¶å…‰..."
)

# æ–°å¢ï¼šè°ƒæ•´åˆ†æå‚æ•°
top_n = st.slider("é€‰æ‹©é«˜é¢‘å…³é”®è¯å±•ç¤ºæ•°é‡", min_value=5, max_value=20, value=10, step=1)
st.divider()

if st.button("ğŸš€ å¼€å§‹åˆ†æ", use_container_width=True):
    if not user_input.strip():
        st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬")
    else:
        # æ ¸å¿ƒåˆ†æ
        text_stats = calculate_text_stats(user_input)
        top_keywords = get_top_keywords(text_stats["çº¯æ–‡æœ¬å†…å®¹"], top_n=top_n)
        sentiment_result = analyze_sentiment(text_stats["çº¯æ–‡æœ¬å†…å®¹"])
        word_segmentation = get_word_segmentation(text_stats["çº¯æ–‡æœ¬å†…å®¹"])
        wordcloud_fig = generate_wordcloud(text_stats["çº¯æ–‡æœ¬å†…å®¹"])
        # ç”Ÿæˆæ–°å¢å›¾å½¢
        keyword_bar_fig = plot_keyword_bar(top_keywords)
        text_pie_fig = plot_text_composition_pie(text_stats)
        sentiment_line_fig = plot_sentiment_reference_line(sentiment_result["æƒ…æ„Ÿå¾—åˆ†"])

        # å±•ç¤ºç»“æœ
        st.success("âœ… åˆ†æå®Œæˆ")
        st.divider()

        # 1. åŸºç¡€ç»Ÿè®¡ï¼ˆæ‰©å……åï¼‰
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ“Š åŸºç¡€æ–‡æœ¬ç»Ÿè®¡")
            st.write(f"å«ç©ºæ ¼æ¢è¡Œæ€»å­—ç¬¦æ•°ï¼š{text_stats['å«ç©ºæ ¼æ¢è¡Œæ€»å­—ç¬¦æ•°']}")
            st.write(f"æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°ï¼š{text_stats['æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°']}")
            st.write(f"çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰ï¼š{text_stats['çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰']}")
        with col2:
            st.subheader("ğŸ“‹ æ‰©å±•ç»Ÿè®¡ä¿¡æ¯")
            st.write(f"å¥å­æ•°ï¼š{text_stats['å¥å­æ•°']}")
            st.write(f"æ ‡ç‚¹ç¬¦å·æ•°ï¼š{text_stats['æ ‡ç‚¹ç¬¦å·æ•°']}")
            st.write(f"å¹³å‡æ¯å¥å­—ç¬¦æ•°ï¼š{round(text_stats['æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°']/text_stats['å¥å­æ•°'], 2)}")

        st.divider()

        # 2. é«˜é¢‘å…³é”®è¯ï¼ˆæ”¯æŒè‡ªå®šä¹‰æ•°é‡ï¼‰+ æŸ±çŠ¶å›¾å±•ç¤º
        st.subheader(f"ğŸ”¤ é«˜é¢‘å…³é”®è¯TOP{top_n}")
        if top_keywords:
            # ç”¨è¡¨æ ¼å±•ç¤ºæ›´æ¸…æ™°
            keyword_data = [[idx, word, count] for idx, (word, count) in enumerate(top_keywords, 1)]
            st.table({"æ’å": [x[0] for x in keyword_data], "å…³é”®è¯": [x[1] for x in keyword_data], "å‡ºç°æ¬¡æ•°": [x[2] for x in keyword_data]})
            # å±•ç¤ºå…³é”®è¯æŸ±çŠ¶å›¾
            st.subheader("ğŸ“Š é«˜é¢‘å…³é”®è¯æŸ±çŠ¶å›¾")
            if keyword_bar_fig:
                st.pyplot(keyword_bar_fig)
        else:
            st.info("ğŸ“Œ æ— æœ‰æ•ˆå…³é”®è¯ï¼ˆæœªç­›é€‰å‡ºé•¿åº¦>1ä¸”éåœç”¨è¯çš„è¯æ±‡ï¼‰")

        st.divider()

        # 3. ä¸­æ–‡åˆ†è¯ç»“æœå±•ç¤º
        st.subheader("âœ‚ï¸ ä¸­æ–‡åˆ†è¯ç»“æœ")
        if word_segmentation:
            st.text_area("åˆ†è¯ç»“æœï¼ˆ| åˆ†éš”ï¼‰", value=word_segmentation, height=100, disabled=True)
        else:
            st.info("ğŸ“Œ æ— æœ‰æ•ˆåˆ†è¯å†…å®¹")

        st.divider()

        # 4. æƒ…æ„Ÿåˆ†æä¸æ–‡æœ¬æ‘˜è¦ + æƒ…æ„Ÿå‚è€ƒå›¾
        col3, col4 = st.columns(2)
        with col3:
            st.subheader("â¤ï¸ æƒ…æ„Ÿå€¾å‘åˆ†æ")
            st.write(f"æƒ…æ„Ÿå¾—åˆ†ï¼š{sentiment_result['æƒ…æ„Ÿå¾—åˆ†']}ï¼ˆ0=è´Ÿé¢ï¼Œ1=æ­£é¢ï¼‰")
            st.write(f"æƒ…æ„Ÿå€¾å‘ï¼š{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
            # æ ¹æ®æƒ…æ„Ÿå€¾å‘æ˜¾ç¤ºä¸åŒæ ·å¼
            if sentiment_result['æƒ…æ„Ÿå€¾å‘'] == "æ­£é¢":
                st.success(f"âœ… æ–‡æœ¬æ•´ä½“åå‘{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
            elif sentiment_result['æƒ…æ„Ÿå€¾å‘'] == "è´Ÿé¢":
                st.error(f"âŒ æ–‡æœ¬æ•´ä½“åå‘{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
            else:
                st.info(f"â„¹ï¸ æ–‡æœ¬æ•´ä½“ä¸º{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
        with col4:
            st.subheader("ğŸ“ æ–‡æœ¬è‡ªåŠ¨æ‘˜è¦")
            summary_list = sentiment_result['æ–‡æœ¬æ‘˜è¦']
            if summary_list:
                for idx, summary in enumerate(summary_list, 1):
                    st.write(f"{idx}. {summary}")
            else:
                st.info("ğŸ“Œ æ— æ³•ç”Ÿæˆæœ‰æ•ˆæ‘˜è¦")
        
        # å±•ç¤ºæƒ…æ„Ÿå¾—åˆ†å‚è€ƒå›¾
        st.subheader("ğŸ“ˆ æƒ…æ„Ÿå¾—åˆ†å‚è€ƒå›¾")
        if sentiment_line_fig:
            st.pyplot(sentiment_line_fig)

        st.divider()

        # 5. æ–‡æœ¬æ„æˆé¥¼å›¾
        st.subheader("ğŸ¥§ æ–‡æœ¬æ„æˆå æ¯”å›¾")
        if text_pie_fig:
            st.pyplot(text_pie_fig)
        else:
            st.info("ğŸ“Œ æ— æ³•ç”Ÿæˆæ–‡æœ¬æ„æˆé¥¼å›¾ï¼ˆæ— æœ‰æ•ˆæ–‡æœ¬æ•°æ®ï¼‰")

        st.divider()

        # 6. è¯äº‘å›¾å±•ç¤º
        st.subheader("â˜ï¸ å…³é”®è¯è¯äº‘å›¾")
        if wordcloud_fig:
            st.pyplot(wordcloud_fig)
        else:
            st.info("ğŸ“Œ æ— æ³•ç”Ÿæˆè¯äº‘å›¾ï¼ˆæ— æœ‰æ•ˆå…³é”®è¯ï¼‰")

        st.divider()
        st.caption("ğŸ’¡ æç¤ºï¼šæ–°å¢æŸ±çŠ¶å›¾ã€é¥¼å›¾ã€æƒ…æ„Ÿå‚è€ƒå›¾åŠŸèƒ½ï¼Œåœç”¨è¯å·²ä¼˜åŒ–ï¼Œæ”¯æŒä¸­æ–‡åˆ†è¯ã€æƒ…æ„Ÿåˆ†æç­‰å¢å¼ºåŠŸèƒ½")