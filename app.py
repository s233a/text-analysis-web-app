import streamlit as st
import requests
from bs4 import BeautifulSoup
import jieba
from collections import Counter

# é¡µé¢é…ç½®
st.set_page_config(page_title="ç½‘é¡µæ–‡æœ¬åˆ†æå·¥å…·", layout="centered")

# ---------------------- æ ¸å¿ƒå‡½æ•° ----------------------
def crawl_web_text(url):
    """çˆ¬å–ç½‘é¡µæ–‡æœ¬ï¼ˆä¼˜åŒ–åçˆ¬+æ­£æ–‡æå–ï¼‰"""
    try:
        headers = {
            "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
        }
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        response.encoding = response.apparent_encoding

        # æå–æ­£æ–‡ï¼ˆè¿‡æ»¤éå†…å®¹æ ‡ç­¾ï¼‰
        soup = BeautifulSoup(response.text, "html.parser")
        for tag in soup(["script", "style", "nav", "footer", "aside", "header", "iframe"]):
            tag.decompose()
        # ä¼˜å…ˆå–articleæ ‡ç­¾ï¼Œæ— åˆ™å–pæ ‡ç­¾é›†åˆ
        article = soup.find("article")
        if article:
            text = article.get_text(strip=True, separator="\n")
        else:
            text = "\n".join([p.get_text(strip=True) for p in soup.find_all("p")])
        
        return text.strip() if len(text.strip()) > 50 else None  # è¿‡æ»¤è¿‡çŸ­æ–‡æœ¬
    except Exception as e:
        st.error(f"çˆ¬å–å¤±è´¥ï¼š{str(e)}")
        return None

def analyze_text(text, top_n=6):
    """æ–‡æœ¬åˆ†æï¼ˆåˆ†è¯+é«˜é¢‘è¯ç»Ÿè®¡ï¼‰"""
    # åˆ†è¯+è¿‡æ»¤åœç”¨è¯
    stop_words = {"çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "åœ¨", "å’Œ", "æœ‰", "å°±", "éƒ½", "è¿™", "é‚£"}
    words = jieba.lcut(text)
    valid_words = [word for word in words if word not in stop_words and len(word) > 1]
    # ç»Ÿè®¡é«˜é¢‘è¯
    if not valid_words:
        return None
    return Counter(valid_words).most_common(top_n)

# ---------------------- é¡µé¢é€»è¾‘ï¼ˆè¡¥å…¨æµç¨‹+æ•°æ®å­˜å‚¨ï¼‰ ----------------------
st.title("ç½‘é¡µæ–‡æœ¬åˆ†æå·¥å…·")

# 1. é€‰æ‹©åˆ†ææ¨¡å¼
mode = st.radio("è¯·é€‰æ‹©åˆ†ææ¨¡å¼", ["ç½‘é¡µURLçˆ¬å–åˆ†æ", "æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬åˆ†æ"])

# 2. ç½‘é¡µURLçˆ¬å–æµç¨‹
if mode == "ç½‘é¡µURLçˆ¬å–åˆ†æ":
    url = st.text_input("è¯·è¾“å…¥æœ‰æ•ˆæ–‡ç« URL", "https://news.sina.com.cn/c/2025-06-20/doc-iahfyqhi8678342.shtml")
    
    # çˆ¬å–æŒ‰é’®ï¼šç‚¹å‡»åå­˜å‚¨æ–‡æœ¬åˆ°session_state
    if st.button("å¼€å§‹çˆ¬å–ç½‘é¡µæ–‡æœ¬"):
        crawled_text = crawl_web_text(url)
        if crawled_text:
            st.session_state["target_text"] = crawled_text  # å­˜å‚¨æ–‡æœ¬
            st.success("âœ… çˆ¬å–æˆåŠŸï¼å¯ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®åˆ†æ")
        else:
            st.warning("âš ï¸ æœªçˆ¬å–åˆ°æœ‰æ•ˆæ–‡æœ¬ï¼ˆå»ºè®®æ›´æ¢å…·ä½“æ–‡ç« URLï¼‰")

# 3. æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬æµç¨‹
else:
    manual_text = st.text_area("è¯·è¾“å…¥å¾…åˆ†ææ–‡æœ¬", height=200)
    if st.button("ç¡®è®¤è¾“å…¥æ–‡æœ¬"):
        if manual_text.strip():
            st.session_state["target_text"] = manual_text.strip()
            st.success("âœ… æ–‡æœ¬å·²å°±ç»ªï¼å¯ç‚¹å‡»ä¸‹æ–¹æŒ‰é’®åˆ†æ")
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬")

# 4. æ–‡æœ¬åˆ†ææµç¨‹ï¼ˆåªæœ‰å­˜åœ¨ç›®æ ‡æ–‡æœ¬æ—¶æ‰å¯ç”¨ï¼‰
if "target_text" in st.session_state:
    top_n = st.slider("é€‰æ‹©é«˜é¢‘å…³é”®è¯å±•ç¤ºæ•°é‡", 3, 20, 6)
    if st.button("å¼€å§‹æ–‡æœ¬åˆ†æ"):
        result = analyze_text(st.session_state["target_text"], top_n)
        if result:
            st.subheader("ğŸ“Š åˆ†æç»“æœ")
            st.write("é«˜é¢‘å…³é”®è¯TOP{}ï¼š".format(top_n))
            for idx, (word, count) in enumerate(result, 1):
                st.write(f"{idx}. {word}ï¼š{count}æ¬¡")
        else:
            st.info("ğŸ“Œ æœªæå–åˆ°æœ‰æ•ˆå…³é”®è¯")
else:
    # æ— æ–‡æœ¬æ—¶ç¦ç”¨åˆ†ææŒ‰é’®ï¼ˆæˆ–æç¤ºï¼‰
    st.button("å¼€å§‹æ–‡æœ¬åˆ†æ", disabled=True)
    st.info("è¯·å…ˆçˆ¬å–/è¾“å…¥æ–‡æœ¬ï¼Œå†è¿›è¡Œåˆ†æ")