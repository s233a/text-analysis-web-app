import streamlit as st
import jieba
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import numpy as np
from snownlp import SnowNLP

# é¡µé¢é…ç½®
st.set_page_config(page_title="å¢å¼ºç‰ˆæ–‡æœ¬åˆ†æå·¥å…·", page_icon="ğŸ“", layout="centered")

# æ‰©å……åœç”¨è¯è¡¨ï¼ˆæ›´å…¨é¢çš„ä¸­æ–‡åœç”¨è¯ï¼‰
STOP_WORDS = {
    "çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ",
    "åœ¨", "å’Œ", "æœ‰", "å°±", "éƒ½", "è¿™", "é‚£", "å…¶",
    "ä¹‹", "äº", "ä»¥", "ä¸º", "è€Œ", "ä¹Ÿ", "å—", "å‘¢",
    "å§", "å•Š", "å“¦", "å—¯", "ç€", "è¿‡", "è¿˜", "å°†",
    "è¦", "ä¼š", "èƒ½", "å¯", "å¯¹", "ä¸", "æˆ–", "åŠ",
    "æ‰€", "æŠŠ", "è¢«", "è®©", "ç»™", "ä½¿", "å¾—", "åˆ°",
    "ä»", "å¾€", "å‘", "æ¯”", "è·Ÿ", "åŒ", "å’Œ", "çš„"
}

# ---------------------- æ ¸å¿ƒå‡½æ•°ï¼ˆæ‰©å……åŠŸèƒ½ï¼‰ ----------------------
def calculate_text_stats(input_text):
    total_with_space = len(input_text)
    pure_text = input_text.replace(" ", "").replace("\n", "")
    total_without_space = len(pure_text)
    
    # æ–°å¢ç»Ÿè®¡é¡¹ï¼šå¥å­æ•°ï¼ˆæŒ‰ã€‚ï¼ï¼Ÿåˆ†å‰²ï¼‰
    sentence_end_chars = "ã€‚ï¼ï¼Ÿï¼›"
    sentence_count = 1  # é»˜è®¤è‡³å°‘1ä¸ªå¥å­
    for char in sentence_end_chars:
        sentence_count += pure_text.count(char)
    
    # æ–°å¢ç»Ÿè®¡é¡¹ï¼šæ ‡ç‚¹ç¬¦å·æ•°
    punctuation_chars = 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹,.!?;:\'"()[]{}<>ã€'
    punctuation_count = sum(1 for char in pure_text if char in punctuation_chars)
    
    # æ–°å¢ç»Ÿè®¡é¡¹ï¼šçº¯æ–‡å­—æ•°ï¼ˆå»é™¤æ ‡ç‚¹ï¼‰
    pure_word_count = total_without_space - punctuation_count
    
    return {
        "å«ç©ºæ ¼æ¢è¡Œæ€»å­—ç¬¦æ•°": total_with_space,
        "æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°": total_without_space,
        "çº¯æ–‡æœ¬å†…å®¹": pure_text,
        "å¥å­æ•°": sentence_count,
        "æ ‡ç‚¹ç¬¦å·æ•°": punctuation_count,
        "çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰": pure_word_count
    }

def get_top_keywords(pure_text, top_n=10):
    """è¿”å›åˆ—è¡¨ï¼Œè€ŒéDataFrameï¼Œå½»åº•è§„é¿ç±»å‹é”™è¯¯"""
    word_list = jieba.lcut(pure_text)
    valid_words = [
        word for word in word_list
        if word not in STOP_WORDS and len(word) > 1
    ]
    if not valid_words:
        return []
    word_count = Counter(valid_words)
    return word_count.most_common(top_n)

def generate_wordcloud(pure_text):
    """ç”Ÿæˆä¸­æ–‡è¯äº‘å›¾"""
    # åˆ†è¯å¹¶è¿‡æ»¤åœç”¨è¯
    word_list = jieba.lcut(pure_text)
    valid_words = " ".join([word for word in word_list if word not in STOP_WORDS and len(word) > 1])
    
    if not valid_words:
        return None
    
    # è®¾ç½®è¯äº‘å‚æ•°ï¼ˆæ”¯æŒä¸­æ–‡æ˜¾ç¤ºï¼‰
    plt.rcParams['font.sans-serif'] = ['DejaVu Sans', 'SimHei', 'WenQuanYi Zen Hei']  # å…¼å®¹ä¸åŒç¯å¢ƒå­—ä½“
    wc = WordCloud(
        width=800,
        height=400,
        background_color="white",
        font_path=None,  # è‡ªåŠ¨é€‚é…ç³»ç»Ÿä¸­æ–‡å­—ä½“
        max_words=100,
        max_font_size=100,
        random_state=42
    ).generate(valid_words)
    
    # è½¬æ¢ä¸ºå›¾ç‰‡æ ¼å¼ä¾›Streamlitå±•ç¤º
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")  # éšè—åæ ‡è½´
    plt.tight_layout()
    return fig

def analyze_sentiment(pure_text):
    """æ–‡æœ¬æƒ…æ„Ÿå€¾å‘åˆ†æï¼ˆåŸºäºSnowNLPï¼‰"""
    if not pure_text:
        return {"æƒ…æ„Ÿå¾—åˆ†": 0.5, "æƒ…æ„Ÿå€¾å‘": "ä¸­æ€§"}
    
    s = SnowNLP(pure_text)
    sentiment_score = s.sentiments  # å¾—åˆ†èŒƒå›´0-1ï¼Œè¶Šæ¥è¿‘1è¶Šæ­£é¢ï¼Œè¶Šæ¥è¿‘0è¶Šè´Ÿé¢
    
    # åˆ¤æ–­æƒ…æ„Ÿå€¾å‘
    if sentiment_score >= 0.7:
        sentiment_tendency = "æ­£é¢"
    elif sentiment_score <= 0.3:
        sentiment_tendency = "è´Ÿé¢"
    else:
        sentiment_tendency = "ä¸­æ€§"
    
    return {
        "æƒ…æ„Ÿå¾—åˆ†": round(sentiment_score, 4),
        "æƒ…æ„Ÿå€¾å‘": sentiment_tendency,
        "æ–‡æœ¬æ‘˜è¦": s.summary(3)  # ç”Ÿæˆ3å¥æ–‡æœ¬æ‘˜è¦
    }

def get_word_segmentation(pure_text):
    """è¿”å›ä¸­æ–‡åˆ†è¯ç»“æœï¼ˆå¸¦åˆ†éš”ç¬¦ï¼‰"""
    word_list = jieba.lcut(pure_text)
    # è¿‡æ»¤åœç”¨è¯ï¼ŒåŒæ—¶ä¿ç•™åŸå§‹åˆ†è¯å±•ç¤º
    filtered_word_list = [word for word in word_list if word not in STOP_WORDS]
    return " | ".join(filtered_word_list)

# ---------------------- é¡µé¢äº¤äº’ï¼ˆæ‰©å……å±•ç¤ºï¼‰ ----------------------
st.title("ğŸ“ å¢å¼ºç‰ˆæ–‡æœ¬åˆ†æWebåº”ç”¨")
st.divider()

user_input = st.text_area(
    "è¯·è¾“å…¥å¾…åˆ†ææ–‡æœ¬",
    height=200,
    placeholder="ç¤ºä¾‹ï¼šä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªšï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥ã€é‡é¤æˆ–è€…éª‘è¡Œï¼Œäº«å—ç¾å¥½çš„å‘¨æœ«æ—¶å…‰..."
)

# æ–°å¢ï¼šè°ƒæ•´åˆ†æå‚æ•°
top_n = st.slider("é€‰æ‹©é«˜é¢‘å…³é”®è¯å±•ç¤ºæ•°é‡", min_value=5, max_value=20, value=10, step=1)
st.divider()

if st.button("ğŸš€ å¼€å§‹åˆ†æ", use_container_width=True):
    if not user_input.strip():
        st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬")
    else:
        # æ ¸å¿ƒåˆ†æ
        text_stats = calculate_text_stats(user_input)
        top_keywords = get_top_keywords(text_stats["çº¯æ–‡æœ¬å†…å®¹"], top_n=top_n)
        sentiment_result = analyze_sentiment(text_stats["çº¯æ–‡æœ¬å†…å®¹"])
        word_segmentation = get_word_segmentation(text_stats["çº¯æ–‡æœ¬å†…å®¹"])
        wordcloud_fig = generate_wordcloud(text_stats["çº¯æ–‡æœ¬å†…å®¹"])

        # å±•ç¤ºç»“æœ
        st.success("âœ… åˆ†æå®Œæˆ")
        st.divider()

        # 1. åŸºç¡€ç»Ÿè®¡ï¼ˆæ‰©å……åï¼‰
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ“Š åŸºç¡€æ–‡æœ¬ç»Ÿè®¡")
            st.write(f"å«ç©ºæ ¼æ¢è¡Œæ€»å­—ç¬¦æ•°ï¼š{text_stats['å«ç©ºæ ¼æ¢è¡Œæ€»å­—ç¬¦æ•°']}")
            st.write(f"æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°ï¼š{text_stats['æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°']}")
            st.write(f"çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰ï¼š{text_stats['çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰']}")
        with col2:
            st.subheader("ğŸ“‹ æ‰©å±•ç»Ÿè®¡ä¿¡æ¯")
            st.write(f"å¥å­æ•°ï¼š{text_stats['å¥å­æ•°']}")
            st.write(f"æ ‡ç‚¹ç¬¦å·æ•°ï¼š{text_stats['æ ‡ç‚¹ç¬¦å·æ•°']}")
            st.write(f"å¹³å‡æ¯å¥å­—ç¬¦æ•°ï¼š{round(text_stats['æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°']/text_stats['å¥å­æ•°'], 2)}")

        st.divider()

        # 2. é«˜é¢‘å…³é”®è¯ï¼ˆæ”¯æŒè‡ªå®šä¹‰æ•°é‡ï¼‰
        st.subheader(f"ğŸ”¤ é«˜é¢‘å…³é”®è¯TOP{top_n}")
        if top_keywords:
            # ç”¨è¡¨æ ¼å±•ç¤ºæ›´æ¸…æ™°
            keyword_data = [[idx, word, count] for idx, (word, count) in enumerate(top_keywords, 1)]
            st.table({"æ’å": [x[0] for x in keyword_data], "å…³é”®è¯": [x[1] for x in keyword_data], "å‡ºç°æ¬¡æ•°": [x[2] for x in keyword_data]})
        else:
            st.info("ğŸ“Œ æ— æœ‰æ•ˆå…³é”®è¯ï¼ˆæœªç­›é€‰å‡ºé•¿åº¦>1ä¸”éåœç”¨è¯çš„è¯æ±‡ï¼‰")

        st.divider()

        # 3. ä¸­æ–‡åˆ†è¯ç»“æœå±•ç¤º
        st.subheader("âœ‚ï¸ ä¸­æ–‡åˆ†è¯ç»“æœ")
        if word_segmentation:
            st.text_area("åˆ†è¯ç»“æœï¼ˆ| åˆ†éš”ï¼‰", value=word_segmentation, height=100, disabled=True)
        else:
            st.info("ğŸ“Œ æ— æœ‰æ•ˆåˆ†è¯å†…å®¹")

        st.divider()

        # 4. æƒ…æ„Ÿåˆ†æä¸æ–‡æœ¬æ‘˜è¦
        col3, col4 = st.columns(2)
        with col3:
            st.subheader("â¤ï¸ æƒ…æ„Ÿå€¾å‘åˆ†æ")
            st.write(f"æƒ…æ„Ÿå¾—åˆ†ï¼š{sentiment_result['æƒ…æ„Ÿå¾—åˆ†']}ï¼ˆ0=è´Ÿé¢ï¼Œ1=æ­£é¢ï¼‰")
            st.write(f"æƒ…æ„Ÿå€¾å‘ï¼š{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
            # æ ¹æ®æƒ…æ„Ÿå€¾å‘æ˜¾ç¤ºä¸åŒæ ·å¼
            if sentiment_result['æƒ…æ„Ÿå€¾å‘'] == "æ­£é¢":
                st.success(f"âœ… æ–‡æœ¬æ•´ä½“åå‘{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
            elif sentiment_result['æƒ…æ„Ÿå€¾å‘'] == "è´Ÿé¢":
                st.error(f"âŒ æ–‡æœ¬æ•´ä½“åå‘{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
            else:
                st.info(f"â„¹ï¸ æ–‡æœ¬æ•´ä½“ä¸º{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
        with col4:
            st.subheader("ğŸ“ æ–‡æœ¬è‡ªåŠ¨æ‘˜è¦")
            summary_list = sentiment_result['æ–‡æœ¬æ‘˜è¦']
            if summary_list:
                for idx, summary in enumerate(summary_list, 1):
                    st.write(f"{idx}. {summary}")
            else:
                st.info("ğŸ“Œ æ— æ³•ç”Ÿæˆæœ‰æ•ˆæ‘˜è¦")

        st.divider()

        # 5. è¯äº‘å›¾å±•ç¤º
        st.subheader("â˜ï¸ å…³é”®è¯è¯äº‘å›¾")
        if wordcloud_fig:
            st.pyplot(wordcloud_fig)
        else:
            st.info("ğŸ“Œ æ— æ³•ç”Ÿæˆè¯äº‘å›¾ï¼ˆæ— æœ‰æ•ˆå…³é”®è¯ï¼‰")

        st.divider()
        st.caption("ğŸ’¡ æç¤ºï¼šåœç”¨è¯å·²ä¼˜åŒ–ï¼Œæ”¯æŒä¸­æ–‡åˆ†è¯ã€æƒ…æ„Ÿåˆ†æã€è¯äº‘ç”Ÿæˆç­‰å¢å¼ºåŠŸèƒ½")