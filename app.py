import streamlit as st
import jieba
from collections import Counter
import matplotlib.pyplot as plt
from wordcloud import WordCloud
import numpy as np
from snownlp import SnowNLP
import os
import platform

# é¡µé¢é…ç½®
st.set_page_config(page_title="å¢å¼ºç‰ˆæ–‡æœ¬åˆ†æå·¥å…·", page_icon="ğŸ“", layout="centered")

# æ‰©å……åœç”¨è¯è¡¨ï¼ˆæ›´å…¨é¢çš„ä¸­æ–‡åœç”¨è¯ï¼‰
STOP_WORDS = {
    "çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "å¥¹", "å®ƒ",
    "åœ¨", "å’Œ", "æœ‰", "å°±", "éƒ½", "è¿™", "é‚£", "å…¶",
    "ä¹‹", "äº", "ä»¥", "ä¸º", "è€Œ", "ä¹Ÿ", "å—", "å‘¢",
    "å§", "å•Š", "å“¦", "å—¯", "ç€", "è¿‡", "è¿˜", "å°†",
    "è¦", "ä¼š", "èƒ½", "å¯", "å¯¹", "ä¸", "æˆ–", "åŠ",
    "æ‰€", "æŠŠ", "è¢«", "è®©", "ç»™", "ä½¿", "å¾—", "åˆ°",
    "ä»", "å¾€", "å‘", "æ¯”", "è·Ÿ", "åŒ", "å’Œ"
}

# ---------------------- æ ¸å¿ƒä¿®å¤ï¼šå¼ºåˆ¶é…ç½®ä¸­æ–‡å­—ä½“ ----------------------
def set_chinese_font():
    """é€‚é…ä¸åŒç³»ç»Ÿçš„ä¸­æ–‡å­—ä½“ï¼Œå¼ºåˆ¶ç”Ÿæ•ˆ"""
    plt.rcParams['axes.unicode_minus'] = False  # è§£å†³è´Ÿå·æ˜¾ç¤ºé—®é¢˜
    system = platform.system()
    if system == "Windows":
        plt.rcParams['font.sans-serif'] = ['SimHei', 'Microsoft YaHei']
    elif system == "Darwin":  # macOS
        plt.rcParams['font.sans-serif'] = ['PingFang SC', 'Heiti SC']
    else:  # Linux
        plt.rcParams['font.sans-serif'] = ['WenQuanYi Zen Hei', 'DejaVu Sans']
    # å…œåº•ï¼šå¦‚æœä»¥ä¸Šå­—ä½“éƒ½ä¸å­˜åœ¨ï¼Œä½¿ç”¨wordcloudå†…ç½®å…¼å®¹é€»è¾‘
    return plt.rcParams['font.sans-serif'][0]

# åˆå§‹åŒ–å­—ä½“
CH_FONT = set_chinese_font()

# ---------------------- æ ¸å¿ƒå‡½æ•°ï¼ˆå…¨é‡ä¿®å¤ï¼‰ ----------------------
def calculate_text_stats(input_text):
    total_with_space = len(input_text)
    pure_text = input_text.replace(" ", "").replace("\n", "")
    total_without_space = len(pure_text)
    
    sentence_end_chars = "ã€‚ï¼ï¼Ÿï¼›"
    sentence_count = 1
    for char in sentence_end_chars:
        sentence_count += pure_text.count(char)
    
    punctuation_chars = 'ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š""''ï¼ˆï¼‰ã€ã€‘ã€Šã€‹,.!?;:\'"()[]{}<>ã€'
    punctuation_count = sum(1 for char in pure_text if char in punctuation_chars)
    pure_word_count = total_without_space - punctuation_count
    
    return {
        "å«ç©ºæ ¼æ¢è¡Œæ€»å­—ç¬¦æ•°": total_with_space,
        "æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°": total_without_space,
        "çº¯æ–‡æœ¬å†…å®¹": pure_text,
        "å¥å­æ•°": sentence_count,
        "æ ‡ç‚¹ç¬¦å·æ•°": punctuation_count,
        "çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰": pure_word_count
    }

def get_top_keywords(pure_text, top_n=10):
    """ä¿®å¤ï¼šå…œåº•ç©ºæ•°æ®ï¼Œç¡®ä¿è¿”å›æ ¼å¼ç¨³å®š"""
    if not pure_text:
        return []
    word_list = jieba.lcut(pure_text)
    valid_words = [
        word for word in word_list
        if word not in STOP_WORDS and len(word) > 1 and word.strip()
    ]
    if not valid_words:
        return []
    word_count = Counter(valid_words)
    return word_count.most_common(top_n)

def generate_wordcloud(pure_text):
    """ä¿®å¤ï¼šå¼ºåˆ¶æŒ‡å®šå­—ä½“ï¼Œå…œåº•ç©ºæ•°æ®"""
    if not pure_text:
        return None
    
    word_list = jieba.lcut(pure_text)
    valid_words = [word for word in word_list if word not in STOP_WORDS and len(word) > 1 and word.strip()]
    if not valid_words:
        return None
    
    valid_words_str = " ".join(valid_words)
    # å¼ºåˆ¶æŒ‡å®šå­—ä½“è·¯å¾„ï¼ˆå…¼å®¹ä¸åŒç¯å¢ƒï¼‰
    wc = WordCloud(
        width=800,
        height=400,
        background_color="white",
        font_path=None if os.name == 'nt' else f"/System/Library/Fonts/{CH_FONT}.ttf",  # é€‚é…mac/win/linux
        max_words=100,
        max_font_size=100,
        random_state=42,
        stopwords=STOP_WORDS  # åŒé‡è¿‡æ»¤åœç”¨è¯
    ).generate(valid_words_str)
    
    fig, ax = plt.subplots(figsize=(10, 5))
    ax.imshow(wc, interpolation="bilinear")
    ax.axis("off")
    plt.tight_layout()
    return fig

def analyze_sentiment(pure_text):
    if not pure_text:
        return {"æƒ…æ„Ÿå¾—åˆ†": 0.5, "æƒ…æ„Ÿå€¾å‘": "ä¸­æ€§", "æ–‡æœ¬æ‘˜è¦": []}
    
    s = SnowNLP(pure_text)
    sentiment_score = s.sentiments
    if sentiment_score >= 0.7:
        sentiment_tendency = "æ­£é¢"
    elif sentiment_score <= 0.3:
        sentiment_tendency = "è´Ÿé¢"
    else:
        sentiment_tendency = "ä¸­æ€§"
    
    return {
        "æƒ…æ„Ÿå¾—åˆ†": round(sentiment_score, 4),
        "æƒ…æ„Ÿå€¾å‘": sentiment_tendency,
        "æ–‡æœ¬æ‘˜è¦": s.summary(3) if len(pure_text) > 10 else ["æ–‡æœ¬è¿‡çŸ­ï¼Œæ— æ³•ç”Ÿæˆæ‘˜è¦"]
    }

def get_word_segmentation(pure_text):
    if not pure_text:
        return "æ— æœ‰æ•ˆæ–‡æœ¬"
    word_list = jieba.lcut(pure_text)
    filtered_word_list = [word for word in word_list if word not in STOP_WORDS and word.strip()]
    if not filtered_word_list:
        return "æ— æœ‰æ•ˆåˆ†è¯ï¼ˆå…¨ä¸ºåœç”¨è¯/æ ‡ç‚¹ï¼‰"
    return " | ".join(filtered_word_list)

def plot_keyword_bar(top_keywords):
    """ä¿®å¤ï¼šå¼ºåˆ¶å­—ä½“ï¼Œç©ºæ•°æ®å…œåº•ï¼Œä¼˜åŒ–æ ‡ç­¾æ˜¾ç¤º"""
    if not top_keywords:
        fig, ax = plt.subplots(figsize=(10, 6))
        ax.text(0.5, 0.5, "æ— æœ‰æ•ˆå…³é”®è¯å¯å±•ç¤º", ha='center', va='center', fontsize=14, fontfamily=CH_FONT)
        ax.axis("off")
        return fig
    
    set_chinese_font()  # ç»˜å›¾å‰é‡æ–°ç¡®è®¤å­—ä½“
    words = [item[0] for item in top_keywords]
    counts = [item[1] for item in top_keywords]
    
    fig, ax = plt.subplots(figsize=(10, 6))
    bars = ax.bar(words, counts, color='#2E86AB', alpha=0.8, edgecolor='#1A5276')
    
    # å¼ºåˆ¶ä¸­æ–‡æ ‡ç­¾æ˜¾ç¤º
    ax.set_xticklabels(words, fontfamily=CH_FONT, fontsize=10, rotation=45, ha='right')
    ax.set_xlabel('é«˜é¢‘å…³é”®è¯', fontfamily=CH_FONT, fontsize=12, fontweight='bold')
    ax.set_ylabel('å‡ºç°æ¬¡æ•°', fontfamily=CH_FONT, fontsize=12, fontweight='bold')
    ax.set_title('é«˜é¢‘å…³é”®è¯å‡ºç°æ¬¡æ•°æŸ±çŠ¶å›¾', fontfamily=CH_FONT, fontsize=14, fontweight='bold', pad=20)
    
    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for bar in bars:
        height = bar.get_height()
        ax.text(bar.get_x() + bar.get_width()/2., height + 0.1,
                f'{int(height)}', ha='center', va='bottom', fontsize=10, fontfamily=CH_FONT)
    
    plt.tight_layout()
    return fig

def plot_text_composition_pie(text_stats):
    """ä¿®å¤ï¼šå¼ºåˆ¶å­—ä½“ï¼Œç©ºæ•°æ®å…œåº•"""
    set_chinese_font()
    pure_word_count = text_stats["çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰"]
    punctuation_count = text_stats["æ ‡ç‚¹ç¬¦å·æ•°"]
    
    if pure_word_count + punctuation_count == 0:
        fig, ax = plt.subplots(figsize=(8, 6))
        ax.text(0.5, 0.5, "æ— æœ‰æ•ˆæ–‡æœ¬æ•°æ®å¯å±•ç¤º", ha='center', va='center', fontsize=14, fontfamily=CH_FONT)
        ax.axis("off")
        return fig
    
    labels = ['çº¯æ–‡å­—', 'æ ‡ç‚¹ç¬¦å·']
    sizes = [pure_word_count, punctuation_count]
    colors = ['#A23B72', '#F18F01']
    explode = (0.05, 0)
    
    fig, ax = plt.subplots(figsize=(8, 6))
    wedges, texts, autotexts = ax.pie(
        sizes, explode=explode, labels=labels, colors=colors,
        autopct='%1.1f%%', shadow=True, startangle=90,
        textprops={'fontsize': 10, 'fontfamily': CH_FONT}
    )
    
    for autotext in autotexts:
        autotext.set_color('white')
        autotext.set_fontweight('bold')
        autotext.set_fontfamily(CH_FONT)
    
    ax.set_title('æ–‡æœ¬æ„æˆå æ¯”é¥¼å›¾ï¼ˆçº¯æ–‡å­—/æ ‡ç‚¹ç¬¦å·ï¼‰', fontsize=14, fontweight='bold', pad=20, fontfamily=CH_FONT)
    plt.tight_layout()
    return fig

def plot_sentiment_reference_line(sentiment_score):
    """ä¿®å¤ï¼šå¼ºåˆ¶å­—ä½“ï¼Œä¼˜åŒ–æ˜¾ç¤º"""
    set_chinese_font()
    
    x = [0, 0.3, 0.7, 1]
    y = [0, 0, 0, 0]
    labels = ['è´Ÿé¢', 'ä¸­æ€§é˜ˆå€¼', 'æ­£é¢é˜ˆå€¼', 'æ­£é¢']
    
    fig, ax = plt.subplots(figsize=(10, 3))
    ax.plot(x, y, color='#C73E1D', linewidth=2, linestyle='--', label='æƒ…æ„Ÿå€¾å‘åˆ†ç•Œçº¿')
    ax.scatter(sentiment_score, 0, color='#2E86AB', s=200, zorder=5, label=f'å½“å‰å¾—åˆ†ï¼š{sentiment_score}')
    
    # å¼ºåˆ¶ä¸­æ–‡æ ‡æ³¨
    for i, label in enumerate(labels):
        ax.text(x[i], 0.05, label, ha='center', va='bottom', fontsize=10, fontweight='bold', fontfamily=CH_FONT)
    
    sentiment_label = "æ­£é¢" if sentiment_score >=0.7 else "è´Ÿé¢" if sentiment_score <=0.3 else "ä¸­æ€§"
    ax.text(sentiment_score, -0.05, sentiment_label, ha='center', va='top', 
            fontsize=11, fontweight='bold', color='red', fontfamily=CH_FONT)
    
    ax.set_xlim(-0.1, 1.1)
    ax.set_ylim(-0.1, 0.1)
    ax.set_xlabel('æƒ…æ„Ÿå¾—åˆ†åŒºé—´', fontsize=12, fontweight='bold', fontfamily=CH_FONT)
    ax.set_title('æƒ…æ„Ÿå¾—åˆ†å‚è€ƒå›¾ï¼ˆ0=è´Ÿé¢ï¼Œ1=æ­£é¢ï¼‰', fontsize=14, fontweight='bold', pad=20, fontfamily=CH_FONT)
    ax.legend(loc='upper right', prop={'family': CH_FONT})
    ax.axis('off')
    plt.tight_layout()
    return fig

# ---------------------- é¡µé¢äº¤äº’ï¼ˆä¿®å¤åï¼‰ ----------------------
st.title("ğŸ“ å¢å¼ºç‰ˆæ–‡æœ¬åˆ†æWebåº”ç”¨ï¼ˆä¿®å¤ç‰ˆï¼‰")
st.divider()

# ç¤ºä¾‹æ–‡æœ¬ï¼ˆæ–¹ä¾¿æµ‹è¯•ï¼‰
DEFAULT_TEXT = """
ä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œé˜³å…‰æ˜åªšï¼Œé€‚åˆå‡ºé—¨æ•£æ­¥ã€é‡é¤æˆ–è€…éª‘è¡Œï¼Œäº«å—ç¾å¥½çš„å‘¨æœ«æ—¶å…‰ã€‚
å…¬å›­é‡Œçš„èŠ±å¼€å¾—ç‰¹åˆ«æ¼‚äº®ï¼Œæœ‰æ¡ƒèŠ±ã€æ¨±èŠ±ã€éƒé‡‘é¦™ï¼Œäº”é¢œå…­è‰²çš„ï¼Œè®©äººå¿ƒæƒ…æ„‰æ‚¦ã€‚
å’Œå®¶äººä¸€èµ·å‡ºé—¨æ¸¸ç©ï¼ŒèŠèŠå®¶å¸¸ï¼Œåƒåƒç¾é£Ÿï¼Œè¿™æ ·çš„å‘¨æœ«å¤ªå¹¸ç¦äº†ã€‚
"""

user_input = st.text_area(
    "è¯·è¾“å…¥å¾…åˆ†ææ–‡æœ¬ï¼ˆå¯ç›´æ¥ä½¿ç”¨ç¤ºä¾‹æ–‡æœ¬æµ‹è¯•ï¼‰",
    height=200,
    placeholder=DEFAULT_TEXT,
    value=DEFAULT_TEXT  # é»˜è®¤å¡«å……ç¤ºä¾‹æ–‡æœ¬ï¼Œæ–¹ä¾¿å¿«é€Ÿæµ‹è¯•
)

top_n = st.slider("é€‰æ‹©é«˜é¢‘å…³é”®è¯å±•ç¤ºæ•°é‡", min_value=5, max_value=20, value=10, step=1)
st.divider()

if st.button("ğŸš€ å¼€å§‹åˆ†æ", use_container_width=True):
    if not user_input.strip():
        st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬")
    else:
        text_stats = calculate_text_stats(user_input)
        top_keywords = get_top_keywords(text_stats["çº¯æ–‡æœ¬å†…å®¹"], top_n=top_n)
        sentiment_result = analyze_sentiment(text_stats["çº¯æ–‡æœ¬å†…å®¹"])
        word_segmentation = get_word_segmentation(text_stats["çº¯æ–‡æœ¬å†…å®¹"])
        wordcloud_fig = generate_wordcloud(text_stats["çº¯æ–‡æœ¬å†…å®¹"])
        keyword_bar_fig = plot_keyword_bar(top_keywords)
        text_pie_fig = plot_text_composition_pie(text_stats)
        sentiment_line_fig = plot_sentiment_reference_line(sentiment_result["æƒ…æ„Ÿå¾—åˆ†"])

        st.success("âœ… åˆ†æå®Œæˆ")
        st.divider()

        # 1. åŸºç¡€ç»Ÿè®¡
        col1, col2 = st.columns(2)
        with col1:
            st.subheader("ğŸ“Š åŸºç¡€æ–‡æœ¬ç»Ÿè®¡")
            st.write(f"å«ç©ºæ ¼æ¢è¡Œæ€»å­—ç¬¦æ•°ï¼š{text_stats['å«ç©ºæ ¼æ¢è¡Œæ€»å­—ç¬¦æ•°']}")
            st.write(f"æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°ï¼š{text_stats['æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°']}")
            st.write(f"çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰ï¼š{text_stats['çº¯æ–‡å­—æ•°ï¼ˆå»æ ‡ç‚¹ï¼‰']}")
        with col2:
            st.subheader("ğŸ“‹ æ‰©å±•ç»Ÿè®¡ä¿¡æ¯")
            st.write(f"å¥å­æ•°ï¼š{text_stats['å¥å­æ•°']}")
            st.write(f"æ ‡ç‚¹ç¬¦å·æ•°ï¼š{text_stats['æ ‡ç‚¹ç¬¦å·æ•°']}")
            st.write(f"å¹³å‡æ¯å¥å­—ç¬¦æ•°ï¼š{round(text_stats['æ— ç©ºæ ¼æ¢è¡Œçº¯å­—ç¬¦æ•°']/text_stats['å¥å­æ•°'], 2)}")

        st.divider()

        # 2. é«˜é¢‘å…³é”®è¯ + æŸ±çŠ¶å›¾
        st.subheader(f"ğŸ”¤ é«˜é¢‘å…³é”®è¯TOP{top_n}")
        if top_keywords:
            keyword_data = [[idx, word, count] for idx, (word, count) in enumerate(top_keywords, 1)]
            st.table({"æ’å": [x[0] for x in keyword_data], "å…³é”®è¯": [x[1] for x in keyword_data], "å‡ºç°æ¬¡æ•°": [x[2] for x in keyword_data]})
        else:
            st.info("ğŸ“Œ æ— æœ‰æ•ˆå…³é”®è¯ï¼ˆæœªç­›é€‰å‡ºé•¿åº¦>1ä¸”éåœç”¨è¯çš„è¯æ±‡ï¼‰")
        
        st.subheader("ğŸ“Š é«˜é¢‘å…³é”®è¯æŸ±çŠ¶å›¾")
        st.pyplot(keyword_bar_fig)

        st.divider()

        # 3. åˆ†è¯ç»“æœ
        st.subheader("âœ‚ï¸ ä¸­æ–‡åˆ†è¯ç»“æœ")
        st.text_area("åˆ†è¯ç»“æœï¼ˆ| åˆ†éš”ï¼‰", value=word_segmentation, height=100, disabled=True)

        st.divider()

        # 4. æƒ…æ„Ÿåˆ†æ + å‚è€ƒå›¾
        col3, col4 = st.columns(2)
        with col3:
            st.subheader("â¤ï¸ æƒ…æ„Ÿå€¾å‘åˆ†æ")
            st.write(f"æƒ…æ„Ÿå¾—åˆ†ï¼š{sentiment_result['æƒ…æ„Ÿå¾—åˆ†']}ï¼ˆ0=è´Ÿé¢ï¼Œ1=æ­£é¢ï¼‰")
            st.write(f"æƒ…æ„Ÿå€¾å‘ï¼š{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
            if sentiment_result['æƒ…æ„Ÿå€¾å‘'] == "æ­£é¢":
                st.success(f"âœ… æ–‡æœ¬æ•´ä½“åå‘{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
            elif sentiment_result['æƒ…æ„Ÿå€¾å‘'] == "è´Ÿé¢":
                st.error(f"âŒ æ–‡æœ¬æ•´ä½“åå‘{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
            else:
                st.info(f"â„¹ï¸ æ–‡æœ¬æ•´ä½“ä¸º{sentiment_result['æƒ…æ„Ÿå€¾å‘']}")
        with col4:
            st.subheader("ğŸ“ æ–‡æœ¬è‡ªåŠ¨æ‘˜è¦")
            summary_list = sentiment_result['æ–‡æœ¬æ‘˜è¦']
            if summary_list:
                for idx, summary in enumerate(summary_list, 1):
                    st.write(f"{idx}. {summary}")
            else:
                st.info("ğŸ“Œ æ— æ³•ç”Ÿæˆæœ‰æ•ˆæ‘˜è¦")
        
        st.subheader("ğŸ“ˆ æƒ…æ„Ÿå¾—åˆ†å‚è€ƒå›¾")
        st.pyplot(sentiment_line_fig)

        st.divider()

        # 5. æ–‡æœ¬æ„æˆé¥¼å›¾
        st.subheader("ğŸ¥§ æ–‡æœ¬æ„æˆå æ¯”å›¾")
        st.pyplot(text_pie_fig)

        st.divider()

        # 6. è¯äº‘å›¾
        st.subheader("â˜ï¸ å…³é”®è¯è¯äº‘å›¾")
        if wordcloud_fig:
            st.pyplot(wordcloud_fig)
        else:
            st.info("ğŸ“Œ æ— æ³•ç”Ÿæˆè¯äº‘å›¾ï¼ˆæ— æœ‰æ•ˆå…³é”®è¯ï¼‰")

        st.divider()
        st.caption("ğŸ’¡ å·²ä¿®å¤ä¸­æ–‡æ˜¾ç¤ºé—®é¢˜ï¼Œå†…ç½®ç¤ºä¾‹æ–‡æœ¬å¯ç›´æ¥æµ‹è¯•")