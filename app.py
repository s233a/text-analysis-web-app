import streamlit as st
import requests
from bs4 import BeautifulSoup
import jieba
from collections import Counter

# é¡µé¢é…ç½®
st.set_page_config(page_title="ç½‘é¡µæ–‡æœ¬åˆ†æå·¥å…·ï¼ˆå¤šå›¾å¯è§†åŒ–ï¼‰", layout="centered")

# ---------------------- æ ¸å¿ƒå‡½æ•° ----------------------
def crawl_web_text(url):
    try:
        # æ¨¡æ‹Ÿæµè§ˆå™¨è¯·æ±‚å¤´ï¼Œé¿å…åçˆ¬
        headers = {"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"}
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()  # æ•è·HTTPé”™è¯¯
        response.encoding = response.apparent_encoding  # è‡ªåŠ¨è¯†åˆ«ç¼–ç ï¼Œè§£å†³ä¸­æ–‡ä¹±ç 

        # è§£æå¹¶æ¸…æ´—HTMLï¼Œæå–æ­£æ–‡
        soup = BeautifulSoup(response.text, "html.parser")
        # è¿‡æ»¤éæ­£æ–‡æ ‡ç­¾
        for tag in soup(["script", "style", "nav", "footer", "aside", "header", "iframe"]):
            tag.decompose()
        # ä¼˜å…ˆæå–articleæ ‡ç­¾ï¼ˆæ–°é—»æ­£æ–‡å¸¸ç”¨æ ‡ç­¾ï¼‰
        article = soup.find("article")
        if article:
            text = article.get_text(strip=True, separator="\n")
        else:
            # æ— articleæ ‡ç­¾åˆ™æå–æ‰€æœ‰pæ ‡ç­¾æ–‡æœ¬
            text = "\n".join([p.get_text(strip=True) for p in soup.find_all("p")])
        
        # è¿‡æ»¤è¿‡çŸ­æ–‡æœ¬
        return text.strip() if len(text.strip()) > 50 else None
    except Exception as e:
        st.error(f"çˆ¬å–å¤±è´¥ï¼š{str(e)}")
        return None

def analyze_text(text, top_n=6):
    # åœç”¨è¯è¡¨ï¼ˆè¿‡æ»¤æ— æ„ä¹‰è¯æ±‡ï¼‰
    stop_words = {"çš„", "äº†", "æ˜¯", "æˆ‘", "ä½ ", "ä»–", "åœ¨", "å’Œ", "æœ‰", "å°±", "éƒ½", "è¿™", "é‚£"}
    # ä¸­æ–‡åˆ†è¯
    words = jieba.lcut(text)
    # è¿‡æ»¤åœç”¨è¯å’Œå•å­—
    valid_words = [word for word in words if word not in stop_words and len(word) > 1]
    if not valid_words:
        return []
    # ç»Ÿè®¡é«˜é¢‘å…³é”®è¯
    return Counter(valid_words).most_common(top_n)

# ---------------------- é¡µé¢é€»è¾‘ ----------------------
st.title("ğŸ“ ç½‘é¡µæ–‡æœ¬åˆ†æå·¥å…·ï¼ˆå¤šå›¾å¯è§†åŒ–ç‰ˆï¼‰")
st.divider()

# åˆ†ææ¨¡å¼é€‰æ‹©
mode = st.radio("è¯·é€‰æ‹©åˆ†ææ¨¡å¼", ["ç½‘é¡µURLçˆ¬å–åˆ†æ", "æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬åˆ†æ"], horizontal=True)

# URLçˆ¬å–åˆ†ææµç¨‹
if mode == "ç½‘é¡µURLçˆ¬å–åˆ†æ":
    url = st.text_input(
        "è¯·è¾“å…¥æœ‰æ•ˆæ–‡ç« URLï¼ˆéé¦–é¡µï¼‰",
        placeholder="ç¤ºä¾‹ï¼šhttps://news.sina.com.cn/c/2025-06-20/doc-iahfyqhi8678342.shtml"
    )
    if st.button("ğŸš€ å¼€å§‹çˆ¬å–ç½‘é¡µæ–‡æœ¬", use_container_width=True):
        crawled_text = crawl_web_text(url)
        if crawled_text:
            st.session_state["target_text"] = crawled_text
            st.success("âœ… çˆ¬å–æˆåŠŸï¼å¯è¿›è¡Œæ–‡æœ¬åˆ†æ")
        else:
            st.warning("âš ï¸ æœªçˆ¬å–åˆ°æœ‰æ•ˆæ–‡æœ¬ï¼ˆè¯·æ›´æ¢å…·ä½“æ–‡ç« URLï¼‰")
# æ‰‹åŠ¨è¾“å…¥æ–‡æœ¬åˆ†ææµç¨‹
else:
    manual_text = st.text_area(
        "è¯·è¾“å…¥å¾…åˆ†ææ–‡æœ¬",
        height=200,
        placeholder="ç¤ºä¾‹ï¼šä»Šå¤©å¤©æ°”å¾ˆå¥½ï¼Œæ˜å¤©å¤©æ°”ä¹Ÿä¸é”™ï¼Œåå¤©é€‚åˆå‡ºé—¨æ•£æ­¥ï¼Œæ•£æ­¥èƒ½æ”¾æ¾å¿ƒæƒ…ï¼Œå¿ƒæƒ…å¥½åšäº‹æ•ˆç‡é«˜"
    )
    if st.button("âœ… ç¡®è®¤è¾“å…¥æ–‡æœ¬", use_container_width=True):
        if manual_text.strip():
            st.session_state["target_text"] = manual_text.strip()
            st.success("âœ… æ–‡æœ¬å·²å°±ç»ªï¼å¯è¿›è¡Œæ–‡æœ¬åˆ†æ")
        else:
            st.warning("âš ï¸ è¯·è¾“å…¥æœ‰æ•ˆæ–‡æœ¬")

# æ–‡æœ¬åˆ†æä¸å¤šå›¾è¡¨å±•ç¤º
if "target_text" in st.session_state:
    # è°ƒæ•´é«˜é¢‘å…³é”®è¯å±•ç¤ºæ•°é‡
    top_n = st.slider("é€‰æ‹©é«˜é¢‘å…³é”®è¯å±•ç¤ºæ•°é‡", 3, 20, 6)
    if st.button("ğŸ“Š å¼€å§‹æ–‡æœ¬åˆ†æ", use_container_width=True):
        top_keywords = analyze_text(st.session_state["target_text"], top_n)
        if top_keywords:
            # æ‹†åˆ†å…³é”®è¯å’Œæ¬¡æ•°åˆ—è¡¨ï¼ˆä¾›æ‰€æœ‰å›¾è¡¨ä½¿ç”¨ï¼‰
            words = [item[0] for item in top_keywords]
            counts = [item[1] for item in top_keywords]

            # 1. æ–‡å­—å±•ç¤ºé«˜é¢‘å…³é”®è¯
            st.subheader("ğŸ”¤ é«˜é¢‘å…³é”®è¯TOP{}".format(top_n))
            for idx, (word, count) in enumerate(top_keywords, 1):
                st.write(f"{idx}. {word}ï¼š{count}æ¬¡")

            # 2. è¡¨æ ¼å±•ç¤ºç»Ÿè®¡è¯¦æƒ…
            st.subheader("ğŸ“‹ å…³é”®è¯ç»Ÿè®¡è¯¦æƒ…")
            st.table([{"å…³é”®è¯": word, "å‡ºç°æ¬¡æ•°": count} for word, count in top_keywords])

            # 3. å¤šå›¾è¡¨å¯è§†åŒ–å±•ç¤ºï¼ˆåˆ†æ å¸ƒå±€ï¼Œæ— æ— æ•ˆå‚æ•°ï¼‰
            st.subheader("ğŸ“ˆ å¤šç»´åº¦æ•°æ®å¯è§†åŒ–")
            # ç¬¬ä¸€æ’ï¼šæŸ±çŠ¶å›¾ï¼ˆçºµå‘ï¼‰ + æ¡å½¢å›¾ï¼ˆæ¨ªå‘ï¼‰
            col1, col2 = st.columns(2)
            with col1:
                st.caption("æŸ±çŠ¶å›¾ï¼ˆçºµå‘ï¼šå…³é”®è¯è¯é¢‘å¯¹æ¯”ï¼‰")
                st.bar_chart({"å…³é”®è¯": words, "å‡ºç°æ¬¡æ•°": counts}, x="å…³é”®è¯", y="å‡ºç°æ¬¡æ•°", color="#1f77b4")
            with col2:
                st.caption("æ¡å½¢å›¾ï¼ˆæ¨ªå‘ï¼šé•¿å…³é”®è¯æ›´æ˜“è¯»å–ï¼‰")
                # äº¤æ¢x/yè½´å®ç°æ¨ªå‘æ¡å½¢å›¾
                st.bar_chart({"å…³é”®è¯": words, "å‡ºç°æ¬¡æ•°": counts}, x="å‡ºç°æ¬¡æ•°", y="å…³é”®è¯", color="#ff7f0e")

            # ç¬¬äºŒæ’ï¼šæŠ˜çº¿å›¾ + é¥¼çŠ¶å›¾ï¼ˆä¿®å¤åï¼Œæ— colorå‚æ•°ï¼‰
            col3, col4 = st.columns(2)
            with col3:
                st.caption("æŠ˜çº¿å›¾ï¼ˆå…³é”®è¯è¯é¢‘è¶‹åŠ¿ï¼‰")
                st.line_chart({"å…³é”®è¯": words, "å‡ºç°æ¬¡æ•°": counts}, x="å…³é”®è¯", y="å‡ºç°æ¬¡æ•°", color="#2ca02c")
            with col4:
                st.caption("é¥¼çŠ¶å›¾ï¼ˆå…³é”®è¯è¯é¢‘å æ¯”ï¼‰")
                # æ„é€ é¥¼å›¾æ•°æ®ï¼ˆå­—å…¸æ ¼å¼ï¼‰
                pie_data = dict(zip(words, counts))
                # ç›´æ¥ä¼ å…¥å­—å…¸ï¼ŒStreamlitè‡ªåŠ¨æ¸²æŸ“é¥¼å›¾
                st.pie_chart(pie_data)
        else:
            st.info("ğŸ“Œ æœªæå–åˆ°æœ‰æ•ˆå…³é”®è¯ï¼ˆæ–‡æœ¬è¿‡çŸ­æˆ–æ— æœ‰æ•ˆè¯æ±‡ï¼‰")
else:
    # æ— æ–‡æœ¬æ—¶ç¦ç”¨åˆ†ææŒ‰é’®
    st.button("ğŸ“Š å¼€å§‹æ–‡æœ¬åˆ†æ", disabled=True, use_container_width=True)
    st.info("â„¹ï¸ è¯·å…ˆçˆ¬å–/è¾“å…¥æ–‡æœ¬ï¼Œå†è¿›è¡Œåˆ†æ")